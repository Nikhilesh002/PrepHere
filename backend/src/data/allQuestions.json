[
  {
    "text": "Let's say you have a partitioned table, where one of the partition has huge amount of data and it is getting stuck. how do you handle this situation and what measures do you take?",
    "summary": "Spark partitions",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Oportun",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do you handle the CDC events during data ingestion? what are some of the best practices used?",
    "summary": "CDC events",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Oportun",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "create table samples (\nsample_date int,\nsample_time int,\ndevice_id int,\nsample_value int\n);\n \ninsert into samples (sample_date,sample_time,device_id,sample_value) values\n(20180701, 1010, 111, 11)\n,(20180701, 1011, 111, 12)\n,(20180701, 1012, 111, 13)\n,(20180701, 1013, 222, 11)\n,(20180701, 1014, 222, 11)\n,(20180701, 1015, 222, 12)\n,(20180701, 1016, 111, 12)\n,(20180701, 1017, 111, 11)\n,(20180701, 1018, 222, 13)\n,(20180701, 1019, 222, 12)\n,(20180701, 1020, 222, 13)\n,(20180701, 1021, 222, 12)\n,(20180701, 1022, 222, 12)\n,(20180701, 1023, 111, 12)\n,(20180701, 1024, 111, 13)\n,(20180701, 1025, 111, 13)\n,(20180701, 1026, 111, 12)\n,(20180701, 1027, 111, 13)\n,(20180701, 1028, 222, 14)\n,(20180701, 1029, 222, 13)\n,(20180701, 1030, 222, 14)\n,(20180701, 1031, 222, 14)\n,(20180701, 1032, 222, 14)\n,(20180701, 1033, 222, 14)\n,(20180701, 1034, 222, 14)\n,(20180701, 1035, 222, 14)\n,(20180701, 1036, 111, 13)\n,(20180701, 1037, 111, 13)\n,(20180701, 1038, 111, 14)\n,(20180701, 1039, 111, 13);\n \n  \nwrite sql to fetch rows with same device_id, same sample values and having sample_time values are consecutive",
    "summary": "Device Stats",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Oportun",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "find the top 5 employees who have the highest average sales per month in the last year (from the current date), considering only those employees who have made at least 10 sales during that period. Include the employee name, average monthly sales amount, and the total number of sales for each employee.\ntables:\nemployees (employee_id, employee_name)\nsales (sale_id, employee_id, sale_date, amount)\n \nemployee_id employee_name\n1\tJohn Smith\n2\tAlice Johnson\n3\tBob Williams\n4\tEva Brown\n5\tMichael Davis\n6\tSarah Wilson\n7\tDavid Garcia\n8\tLinda Rodriguez\nsale_id   employee_id  sale_date amount\n1\t1\t2023-08-15\t1500\n2\t1\t2023-09-20\t2000\n3\t1\t2023-10-10\t1800\n4\t1\t2023-11-05\t2200\n5\t1\t2023-12-12\t1900\n6\t1\t2024-01-18\t2100\n7\t1\t2024-02-25\t2300\n8\t1\t2024-03-01\t1700\n9\t1\t2024-04-07\t2400\n10\t1\t2024-05-14\t2000\n11\t1\t2024-06-21\t1800\n12\t1\t2024-07-28\t2500\n13\t2\t2023-09-01\t1000\n14\t2\t2023-10-05\t1200\n15\t2\t2023-11-10\t1500",
    "summary": "Highest average sales",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Oportun",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Find the companies who have at least 2 users who speak both English and German in\npyspark\n\nData:\ndata = [(A, 1, English),\n(A, 1, German),\n(A, 2, English),\n(A, 2, German),\n(A, 3, German),\n(B, 1, English),\n(B, 2, German),\n(C, 1, English),\n(C, 2, German)]\n\nschema =(company_id, user_id, language)",
    "summary": "Languages spoken",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "EPAM Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "what are some of the files generated when a spark job is executed and what info does they contain?",
    "summary": "Spark files",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "EPAM Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "Load records into the respective file based on the filename value and add the category\ncount for each file\n\nInput file:\nID, Name, catgry, fileName\n101,emp1, A, abc.txt\n102,emp2, B, bbc.txt\n103,emp3, C, abc.txt\n104,emp4, A, bbc.txt\n105,emp5, C, abc.txt\noutput files:\nabc.txt\n101,emp1, A\n103,emp3, C\n105,emp5, C\nA:1,B:0,c:2\nbbc.txt\n102,emp2, B\n104,emp4, A\nA:1, B:1,c:0",
    "summary": "Filename category",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "EPAM Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "How do you execute notebook based on the file arrival?",
    "summary": "Notebook execution",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Persistent Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "Write a Pyspark code to convert the following input dataframe into output dataframe?\n\nInput Structure:\nID | customerID\n1 12\n1 23\n1 34\n1 54\n2 45\n2 60\n\nOutput Structure:\nID customerID\n1 [12,23,34,54]\n2 [45,60]",
    "summary": "Pyspark dataframe",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Persistent Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "Need to fix matches between country but there should not be no repetition match.\n\nInput:\n\nIndia\nPaksitan\nAustralia\nNew Zealand\n\nOutput:\nIndia vs Pakistan\nIndia Vs Australia\nIndia vs new Zealand\nPakistan vs Australia\nPakistan vs New Zealand\nAustralia vs New Zealand",
    "summary": "Match Fix",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Persistent Systems",
    "yoe": "12-15",
    "role": "Data Architect"
  },
  {
    "text": "1. what are init scripts\n2. difference between job cluster and normal cluster\n3. difference between data lake and delta lake\n4. what is unity catlog",
    "summary": "Spark Theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "The Math Co",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "given schema with tables customers(cid,name) products(pid,pname,price) and sales(saleid,cid,pid,product,qty,store)\n1. find names of customer who have not made any purchase\n2. write query to find top 2 customers with the highest total order amount\n3. find the name of product with the lowest quantity\n\n\n\ncid name\n1 Alice\n2 Bob\n3 Carol\n4 David\n5 Eve\n\npid pname price\n101 Laptop 1200\n102 Mouse 25\n103 Keyboard 75\n104 Monitor 300\n105 Webcam 50\n\nsaleid cid pid qty store\n1 1 101 1 Store A\n2 1 102 2 Store B\n3 2 103 1 Store A\n4 2 101 1 Store C\n5 1 104 1 Store B\n6 3 102 5 Store A\n7 3 105 2 Store C\n8 1 103 1 Store A",
    "summary": "Sales stats",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "The Math Co",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "python function to sort array\n2)given this input i= [0,1,0,5,4,0]\n#op: [1,4,5,0,0,0]",
    "summary": "Sort array",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "The Math Co",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "given some json data and Find out the value of category_name.\n\nj={'invalid_data': {'gtin': []}, 'valid_data': [{'brand_name': 'SHEBA', 'category': {'category_name': 'CAT FOOD', 'segments_name': 'CAT TREATS', 'subcategory_name': 'CAT TREATS'} }] }",
    "summary": "json parsing",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "The Math Co",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You have 2 tables namely tbl_a and tbl_b. Perform left join, inner join, full outer join on both the tables and give me the results when tbl_a data is selected\n\nTbl_a : 1 , 2, 1 NULL\n\nTbl_b: 1, 1, NULL",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Databricks",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Find the max length of the substring which doesn't have any repeating characters for a given string \"str\"\n\nEX. Str=\"abcbdcab\" output: len=4 (\"dcab\")\nStr=\"bbbbb\" output: len=1 (\"b\")",
    "summary": "Max substring length",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Databricks",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given two tables, Customers and Products, find all customers who spent less in the current month than they did in the previous month.\n\nSchema:\n\n    Customers:\n        Customer_id (INT, Primary Key)\n        User_name (VARCHAR)\n        City (VARCHAR)\n    Products:\n        customer_id (INT, Foreign Key referencing Customers.Customer_id)\n        product_id (INT)\n        purchase_date (DATE)\n        Money_spent (DECIMAL)\n\nSample Data:\n\nCustomers Table:\nCustomer_id\tUser_name\tCity\n1\t   Alice\tNew York\n2\tBob\tLos Angeles\n3\tCarol\tChicago\n4\tDavid\tHouston\n\nProducts Table:\ncustomer_id\tproduct_id\tpurchase_date\tMoney_spent\n1\t101\t2023-10-15\t100\n1\t102\t2023-10-20\t50\n1\t103\t2023-11-05\t75\n1\t104\t2023-11-10\t25\n2\t201\t2023-10-01\t200\n2\t202\t2023-11-15\t150\n3\t301\t2023-10-25\t30\n3\t302\t2023-11-20\t40\n4\t401\t2023-10-01\t100\n4\t402\t2023-11-15\t200",
    "summary": "Money spent less",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Databricks",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You are given two tables, TableA and TableB. Perform the following SQL operations and explain the results:\n\nTable1\tTable2\n1\t1\n1\t1\n1\t2\n2\t3\nNULL\t3\n4\tNULL",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "HCLTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SaleID\tCustomerID\tProductID\tSaleAmount\tSaleDate\n1\t101\t201\t500\t2024-02-01\n2\t102\t202\t1500\t2024-05-15\n3\t101\t203\t700\t2024-08-03\n4\t103\t201\t200\t2024-10-19\n5\t101\t201\t300\t2024-01-07\n6\t104\t202\t1200\t2024-03-25\n7\t103\t203\t400\t2024-07-11\nFind the productID which was sold more than other products last 6 months?",
    "summary": "Product Sales",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "HCLTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a table Employee with a column FullName that contains names in the format \"First Last\". Some names only have a first name, and others have both a first and a last name. Write an SQL query to:\n\nSplit the FullName into two separate columns:\nFirstName (everything before the first space).\nLastName (everything after the first space, or NULL if there is no second name).\nEmployee Table\nEmployeeID\tFullName\n1\tJohn Doe\n2\tAlice Johnson\n3\tBob\n4\tCarol Lee",
    "summary": "User Names",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "HCLTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a Python function find_missing_number that takes a list of integers from 1 to n with one missing number, where n is the length of the list plus one. The function should return the missing number.\n\nExample:\npython\nfind_missing_number([1, 2, 4, 6, 3, 7, 8])",
    "summary": "Missing number",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between where and group by in SQL?",
    "summary": "GroupBy function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Is it possible to do window function without partition by or order by?",
    "summary": "Window function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between count(*), count(1), count(column)?",
    "summary": "Count() function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Question:\nYou are given two tables, Orders and Products. The Orders table records each order made by a customer, and the Products table records product details.\n\nTables:\nOrders Table:\n\nOrderID\tCustomerID\tProductID\tOrderDate\tQuantity\n1\t101\t201\t2023-01-10\t2\n2\t102\t202\t2023-01-15\t1\n3\t101\t203\t2023-02-05\t3\n4\t103\t201\t2023-03-20\t4\n5\t104\t202\t2023-02-25\t1\nProducts Table:\n\nProductID\tProductName\tPrice\n201\tProduct A\t500\n202\tProduct B\t1500\n203\tProduct C\t700\nTask:\nWrite an SQL query to find the total revenue generated by each product in the first quarter of 2023 (from January 1, 2023, to March 31, 2023).\n\nReturn the following columns:\n\nProductID\nProductName\nTotalRevenue\nThe result should be sorted by TotalRevenue in descending order.",
    "summary": "Revenue Generated",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Question:\nYou are given two tables, Employees and Departments. The Employees table contains information about employees, including their department, and the Departments table contains information about each department.\n\nTables:\nEmployees Table:\n\nEmployeeID\tEmployeeName\tDepartmentID\tSalary\n1\tJohn Doe\t101\t50000\n2\tAlice Smith\t102\t60000\n3\tBob Johnson\t101\t55000\n4\tCarol Lee\t103\t70000\n5\tDave Brown\t102\t65000\nDepartments Table:\n\nDepartmentID\tDepartmentName\n101\tIT\n102\tHR\n103\tMarketing\nTask:\nWrite an SQL query to find the average salary of employees in each department, but only for departments where the average salary is greater than 55,000.\n\nReturn the following columns:\n\nDepartmentID\nDepartmentName\nAverageSalary\nThe result should be sorted by AverageSalary in descending order.",
    "summary": "Average employee salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Question:\nYou are given two tables, Orders and Customers. The Orders table contains information about customer orders, and the Customers table contains information about customers.\n\nTables:\nOrders Table:\n\nOrderID\tCustomerID\tOrderDate\tOrderAmount\n1\t101\t2023-01-10\t500\n2\t102\t2023-01-15\t1500\n3\t103\t2023-02-05\t700\n4\t101\t2023-03-20\t200\n5\t104\t2023-03-25\t1200\n6\t102\t2023-04-10\t300\nCustomers Table:\n\nCustomerID\tCustomerName\tCity\n101\tJohn Doe\tNew York\n102\tAlice Smith\tLos Angeles\n103\tBob Johnson\tChicago\n104\tCarol Lee\tSan Francisco\nTask:\nWrite an SQL query to find the total order amount for each customer who has placed orders in both January and February 2023.\n\nReturn the following columns:\n\nCustomerID\nCustomerName\nTotalOrderAmount\nThe result should be sorted by TotalOrderAmount in descending order.",
    "summary": "Customers order amount",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to find the topmost manager (head) for each employee in the company.\n\nCREATE TABLE Employees (\n    EmployeeID INT,\n    EmployeeName STRING,\n    ManagerID INT\n);\n\nINSERT INTO Employees (EmployeeID, EmployeeName, ManagerID)\nVALUES\n    (1, 'John', NULL),       -- Topmost Manager 1\n    (2, 'Alice', 1),\n    (3, 'Bob', 1),\n    (4, 'Carol', 2),\n    (5, 'Dave', 3),\n    (6, 'Eve', 4),\n    (7, 'Michael', NULL),    -- Topmost Manager 2\n    (8, 'Sarah', 7),\n    (9, 'Tom', 7),\n    (10, 'Lily', 8);\n\nHint - For Dave topmost manager head is John, For Lily topmost manager head is Michael",
    "summary": "Manager for each employee",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "PepsiCo",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find Number of rows returned after joining two datasets by inner join, left outer join, right outer join, full outer join, cross join\n\nDataSet A\n1\n1\n1\nNULL\n\nDataSet B\n1\n1\nNULL\nNULL",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "PepsiCo",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How would you design an alerting mechanism in Azure Data Factory for long running Data Factory Pipelines.",
    "summary": "Alerting mechanism",
    "difficulty": "Hard",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "PepsiCo",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What all optimization techniques have you incorporated in your databricks project ? \nWhat is liquid Clustering ?",
    "summary": "Optimzation techniques",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How would you implement SCD-2 in Pyspark and how would you handle scenarios where we need to delete record in a dimension table.",
    "summary": "SCD-2 records",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Implement map, filter, lambda functions on a python list.\nl = [1,2,3,4,5,6]",
    "summary": "Lambda function",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write SQL query to delete duplicate records from a dataset.",
    "summary": "Delete duplicate records",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write sql query to find second highest salary from each department.",
    "summary": "Second highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Implement SCD - Slowly changing dimension type 2 in scala/python",
    "summary": "SCD Implementation",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "If there are 10 failed Pipelines in monitor instead of rerunning one by one how will you rerun only failed Pipelines all together.",
    "summary": "Pipeline failure scenario",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Consider you are having orders and customers table. \nFind out the customers who have spent most on their orders in the last month in SQL as well as Pyspark.",
    "summary": "Customers spent",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Though Spark does processing of data in memory why cache is required?",
    "summary": "Spark cache",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Can you write a SQL query to find the first highest salary as well as the lowest salary within each department?",
    "summary": "Lowest salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is Change Data Capture? How did you implemented it in your project?\nHow the incremental load is implemented in the silver layer?",
    "summary": "CDC implementation",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do you handle missing data or corrupted data in your project?",
    "summary": "Missing Data",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do you load a file where the columns keeps changing?",
    "summary": "Column changes",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a query to show customer_id for the customers that did not have any orders in 2023 from the customers table.",
    "summary": "Customer Orders",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Consider we have table like this named market. \n \nMarketid\t         Item\t         Amount\tType\n1\t\t\tA\t\t50\t\tCurrent\n1\t\t\tB\t\t100\t\tCurrent\n1\t\t\tC\t\t60\t\tCurrent\n2\t\t\tA\t\t100\t\tCurrent\n2\t\t\tB\t\t60\t\tCurrent\n3\t\t\tC\t\t70\t\tCurrent\n4\t\t\tA\t\t50\t\tCurrent\n4\t\t\tC\t\t20\t\tCurrent\n1\t\t\tA\t\t40\t\tPrevious\n1\t\t\tB\t\t80\t\tPrevious\n1\t\t\tC\t\t70\t\tPrevious\n2\t\t\tA\t\t80\t\tPrevious\n2\t\t\tB\t\t50\t\tPrevious\n3\t\t\tC\t\t60\t\tPrevious\n4\t\t\tA\t\t40\t\tPrevious\n\nAlso note that formula for contribution looks something like this. \nContribution = ((sum of current amount by item for a market/total sum of current amount by market) â€“ (sum of previous amount by item for a market /total sum of previous amount by market))*100\n\nWrite a SQL query to find out the contribution which comes from each item.",
    "summary": "Contribution from each item",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kantar",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Consider we have input which looks this.\n\n1|A|25|X|\n2|B|26|Y|\n3|C|27|Z\n\nWrite a code in python to get a output which looks like below. Note that '|' is also a alphabet.\n\nColumn1\tColumn2\tColumn3\tColumn4\n1\t\tA\t \t25\t\tX\n2\t\tB\t \t26\t\tY\n3\t\tC\t \t27\t\tZ\n\nFollow up question: How would you do it in pyspark?",
    "summary": "Data ordering",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kantar",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Consider we have table department like below.\nUser\tDepartment\nA\tX\nA\tX\nB\tY\nC\tX\nC\tY\nD\tX\nD\tX\n\nHow would you remove duplicates in SQL? \nNote that User D has Department X in both the rows. This case should also be handled.",
    "summary": "Duplicates in department",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kantar",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How would you automate the process of adding new columns into databricks notebook without manually changing it on Databricks notebook?",
    "summary": "Column addition",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kantar",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1.stored procedure vs Functions\n2.can use insert query inside SQL function\n3.types of join\n4. cross join vs self join\n5. what is window function and window function vs aggregated functions",
    "summary": "Basic SQL questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Aays Analytics",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Input Table:\nnum\n1\n2\n3\n4\n\nExpected result:\n\n1\n2\n2\n3\n3\n3\n4\n4\n4\n4",
    "summary": "Numbers repition",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Aays Analytics",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "table:customer\n--emp_no\n--emp_name\n--salary\n--dep\n\nwrite a query to get department with at least 5 employee and salary in greater than 10000\n\nin both SQL and pyspark",
    "summary": "Department details",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Aays Analytics",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "difference between dict and set\nturple and list",
    "summary": "Python dict",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "how to handle errors in python\nerror handling (try,except,finally)",
    "summary": "Error handling",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "what is lambda function and give example with code",
    "summary": "Lambda Function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "difference between group by and having",
    "summary": "GroupBy function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "why we need cloud service like azure , aws\nwhat is etl\nOLAP vs OLTP\nwhat is data warehouse\nbatch vs streaming processing\nwhat is NoSQL\nwhat is distributed computing",
    "summary": "ETL Questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "onprimes database vs cloud database",
    "summary": "Onprem database",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "write a sql query to update a record in table for the emp_id 225 with salary 15000\n\nwhat is corelated subquery ,write a query using it",
    "summary": "corelated subquery",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Employee\n--employeeid\n--name\n--department\n\nsalaries\n--id\n--employeeid\n--date\n--amount\n\nwrite a sql query to get top 2 department where employees get highest total salary in last 6 month",
    "summary": "Top employee salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Techmango Technology Services Private Limited",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Find the total number of orders placed by each customer, excluding orders placed in June.\n\n\nTables :::::\n\n\nCustomers Table ::\n\nname,cust_id\nTeena,1\nSeema,2\nRevi,3\nGany,4\n\nOrders table ::::\n\ncust_id,order_date\n1,2015-12-18 \n1,2011-10-11\n2,2012-06-19\n2,2015-09-07\n2,2018-11-09\n3,2014-03-14",
    "summary": "Exclude orders in june",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "AST Space Mobile",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a query to find the department with the highest average salary for employees who have been with the company for more than 2 years?\n\n\nEmp Table ::\n\nSalary,DeptID,HireDate\n10000,1,2015-12-18 \n35000,2,2021-12-18\n27000,3,2020-12-18\n14000,3,2021-12-18\n\nDept Table ::\n\nDeptID,DeptName \n1,HR\n2,Finance\n3,Logistics\n4,Sales",
    "summary": "Highest average salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "AST Space Mobile",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How to avoid OOM issues spark?",
    "summary": "OOM spark",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "AST Space Mobile",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain Caching in Spark Streaming ?",
    "summary": "Spark streaming",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Corresponding to each salesman, display their total customer count as represented below.\n\nSALESMAN      CUSTOMER\nAria   Nexoraa\nAria   SwiftKart\nMila   Zenoviya\nMila   WitalSpring\nMila   PureWive\nHenry  EkoNowa\nHenry  GreenPulze\nZayden SolarNext",
    "summary": "Salesman total count",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "What is shuffling in Spark? When does it occur ?",
    "summary": "Shuffling in spark",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Identify the countries present in multiple continents\n\nCOUNTRY       CONTINENT\nEgypt  Africa\nPoland Europe\nIndia  Asia\nTurkiye       Asia\nTurkiye       Europe\nKazakhstan    Asia\nKazakhstan    Europe\nIndia  Asia",
    "summary": "Countries in continents",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "We have a list and tuple, create a dict with key-value pair? which one you use as key?which one you use as value?",
    "summary": "Create a dictionary",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. What is SCD? Different types of SCD?\n2. Difference b/w map and filter in python?\n3. What is generator in python?\n4. What is magic function in python?\n5. What is difference between generator function and normal function in python?\n6. Difference between NVL, NVL2 and Coalesce ?\n7. Difference between rank and dense_rank?\n8. Difference between star schema and galaxy schema?\n9. How to optimize any spark job?\n10. What is broadcast join?",
    "summary": "DE theory questions",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How to handle skewed data in spark?",
    "summary": "Data skewness",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "eid, name, desig, date \n1, a, e, 11/12/2024\n2, b, e, 12/12/2024\n1, a, m, 12/12/2024\nWrite a SQL query to find eid who have latest designation.",
    "summary": "Latest designation",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "eid, name, desig, date -- emp(dimension table)\n1, a, e, 11/12/2024\n\neid, name, desig, date -- emp_stage(staging table)\n2, b, e, 12/12/2024\n1, a, m, 12/12/2024\nWrite a incremental load query(Actually insert/update query) to insert data from emp_stage to emp. Scenerio was like latest data is coming, you have to keep on adding it.",
    "summary": "Incremental data load",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "a b c\nb c d \nc d e \nAbove data is present in txt file. Write a word count program for this in python/spark.",
    "summary": "word count",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "name = 'a b c' -- Write a code which will always provide last name in python?",
    "summary": "Print last name",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is persist in spark?",
    "summary": "Persist in spark",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "a = [1,1,2,2,3,3,4,4,4,5] -- Write a python program to find unique values without using any pre-defined func.",
    "summary": "UDF for unique values",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Bluepi",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. What is the Project Architecure/pipeline?\n2. What is the different source of system in project?\n3. What is the size of data\n4. Which file format are you using in the project?\n5. Where you are using Hive in you project?",
    "summary": "Project Questions",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is the count of inner, left, right and outer join for below.\n      t1 - c1 - 1,1,1,2,2,3, null\n      t2 - c2 - 1,1,2,2,4,null,null\n      where t1 is table and column present in t1 is c1 & t2 is table and column present in t2 is c2.",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "In table t1, 3 columns are present a, b, c with no primary key.\n       Write a SQL query to find the duplicate records.",
    "summary": "Find duplicates",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "There are several csv files are present in HDFS folder with state names like bihar.csv, kerala.csv etc.\nWrite a pyspark program where you have to add one more column with column name as \"State\" in each csv file and data of \"State\" column should be the filename.",
    "summary": "Display the states based on filename",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Table-tmp\nInput\nid\na\nb\nc\nd\ne\nf\n\nOutput\nid        column\na        [a,b,c,d,e,f]\nb        [a,b,c,d,e]\nc        [a,b,c,d]\nd        [a,b,c]\ne        [a,b]\nf        [a]",
    "summary": "Python pattern print",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "We have applied partition in a hive table. But when we are quering the data on that partition, data is not coming/showing. How you will resolve this issue so that I can see the data?",
    "summary": "Data is not visible",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a python list,\nlst = [3,2,1,4,5,6,7,8,9,10]\n1. even_list = [2,4,6,8,10]\n2. odd_list = [3,1,5,7,9]\n3. sort both lists - [2,4,6,8,10], [1,3,5,7,9]\n4. multiply corresponding elements and sum them: (2*1 + 3*4 + 5*6 + 7*8 + 9*10 = 190)3,",
    "summary": "Arithmetic operations on lists",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are attributes & measures? Can a dimension table have measures?",
    "summary": "Dimensional modelling",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the table,\n  \nName\tSub1\tSub2\nX\t34\t45\nY\t45\t56\n\nConvert it into\nName\tSub\tMarks\nX\tSub1\t34\nX\tSub2\t45\nY\tSub1\t45\nY\tSub2\t56",
    "summary": "Data conversion",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a string, remove all characters that appear before the first digit. The resulting string should start with the first digit and include everything afterward.\nInput: 98271sdfs98\nOutput: 98271sdfs98\n\nInput: abex232cdes\nOutput: 232cdes",
    "summary": "Remove the characters before first digit",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is the surrogate key?",
    "summary": "Surrogate key",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a Python lambda function that checks whether a given number is even or odd. The function should return True for even numbers and False for odd numbers.",
    "summary": "Lamba function for numbers",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q1.Pyspark coding problem\n================================\nI have 3 dataframes/tables named tasks, user and driver_city consisting of following columns \nfor tasks-task_id,no_of_tasks,task_cost \nfor user- user_id,user_city,distance_km\nfor driver_city driver_id,driver_city,start_time,end_time\ngive me pyspark ad SQL code to get amount earned by driver in a day,but the condition is cut off time is 5pm\"",
    "summary": "Amount earned by driver",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Indium Software",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q2.Coding\n============\n data = [(\"\"2023-01-01\"\", \"\"AAPL\"\", 150.00), (\"\"2023-01-02\"\", \"\"AAPL\"\",\n 155.00), (\"\"2023-01-01\"\", \"\"GOOG\"\", 2500.00), (\"\"2023-01-02\"\", \"\"GOOG\"\",\n 2550.00), (\"\"2023-01-01\"\", \"\"MSFT\"\", 300.00), (\"\"2023-01-02\"\", \"\"MSFT\"\",\n 310.00)]\n create dataframe in pyspark\n find avg. stock value on daily basis for each stock\n find max avg stock value of each stock",
    "summary": "Average stock value",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Indium Software",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "If some job failed with out of memory error in production, what will be your approach to debug that",
    "summary": "Failed job in production",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Indium Software",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q1.Pyspark coding\n==============\nEmp Id\temp_name\tmanagerid\tsalary\n1\tDavid\t3\t100\n2\tSam\t1\t200\n3\tJeff\t3\t2000\n4\tJacob\t4\t2000\nreturn managerid,Manager_name,Emp_id,emp_name, \nsalary where salary is the 2nd highest for employees under each manager.",
    "summary": "Second highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q2.Pyspark coding\n==============\nI have data frame which consists of following columns,give me pyspark code to get most purchased product\norder_id, product_id, quantity, price, cust_id\n\norder_schema=(order_id int, product_id int, quantity int, price int, cust_id int)",
    "summary": "Most purchased product",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "If your spark job is running slow how would you approach to debug it",
    "summary": "Slowly running spark job",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a python code, to find the sum of the numbers which do not have duplicates (i.e 1,3,6 => 10)\nnums = [1,2,3,2,7,7,7,6]",
    "summary": "Sum of non-duplicates",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "Intraedge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Table A\ncol1\n1\nnull\n1\n\nTable B\ncol1\n1\n1\n1\n\nselect count(*) from A left join B on A.col1=B.col1",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "Intraedge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "specify where tranformations & actions are happening and explain\n\ndf1=spark.read.csv('sample.csv')\ndf2=df1.repartition(2)\ndf3=df2.where(age<40).select(age,gender,country).groupBy(Country).count()\ndf3.collect()",
    "summary": "Spark transformations",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "Intraedge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Emp table\n\n| empname | location  | year | vaccination | infected |\n| ------- | --------- | ---- | ----------- | -------- |\n| A       | Delhi     | 2020 | No          | No       |\n| A       | Delhi     | 2021 | Yes         | Yes      |\n| A       | Delhi     | 2022 | Yes         | No       |\n| B       | Delhi     | 2020 | No          | Yes      |\n| B       | Delhi     | 2021 | Yes         | No       |\n| B       | Delhi     | 2022 | Yes         | Yes      |\n| C       | Bangalore | 2020 | Yes         | Yes      |\n| C       | Bangalore | 2021 | No          | No       |\n| C       | Bangalore | 2022 | Yes         | Yes      |\n| D       | Bangalore | 2020 | Yes         | No       |\n| D       | Bangalore | 2021 | Yes         | Yes      |\n| D       | Bangalore | 2022 | Yes         | No       |\n\nWrite a SQL query to fetch the empname those who were vaccinated a particular year but got infected the immediate next year.",
    "summary": "Employee who was vacinated",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "Intraedge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "1.\tExplain partitioning & clustering in Bigquery\n2.\tHow will you decide which column to be used for partitioning & clustering\n3.\tCan we create index in Bigquery?\n4.\tOptimization techniques in Pyspark jobs\n5.\tData modeling in DW",
    "summary": "Spark theory questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "Intraedge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "GCP:\n1.\tWhat is dataproc?\n2.\tClasses used in pyspark job\n3.\tBasic configurations details for creating a dataproc cluster\n4.\tHow to load data without spinning clusters using dataproc\n5.\tDifferent sections in pyspark code. How to initiate spark shell?\n6.\tHow will you check logs in dataflow and dataproc\n7.\tHow to submit a dataproc job",
    "summary": "DE theory questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "CTS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Airflow:\n\n1.\tWhat is Apache Airflow?\n2.\tHow will you create a new dag code to be able to reflect in Apache Airflow?\n3.\tWhere can you view the code in Airflow? Can we edit the code in Airflow UI?\n4.\tWhere will you check logs in Airflow?\n5.\tHow will you send notification on dag failure?\n6.\tWhat are the different operators you have used in dag?\n7.\tWhat are the main sections in dag?\n8.\tHow will you set dependencies between tasks in dag?",
    "summary": "Airflow questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "CTS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Employee:\nemployee_id| emp_name |dept_id   |salary\n-------------|-------------|------------|------\n\t1\t   |\tAlice       |101\t         |80000\n\t1\t   |\tBob         |101\t         |75000\n\t1\t   |\tCharlie    |101\t         |90000\n\t1\t   |\tDavid      |102\t         |60000\n\t1\t   |\tEva         |102\t         |65000\n\t1\t   |\tFrank      |102\t         |62000\n\t1\t   |\tGrace      |103\t         |70000\n\t1\t   |\tHelen      |103\t         |82000\n\t1\t   |\tIan           |103\t         |91000\n\nWrite a query to find the top 2 highest-paid employees from each department.",
    "summary": "Highest paid employees",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "CTS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a python function to count number of vowels in a word.\n\nO/P:\nWord: apple, Vowels: 2\nWord: banana, Vowels: 3\nWord: grape, Vowels: 2\nWord: sky, Vowels: 0\nWord: rhythm, Vowels: 0\nWord: umbrella, Vowels: 3",
    "summary": "Count the vowels",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "CTS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do you handle the data skew in spark jobs",
    "summary": "Data skewness",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Bitwise",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given The data of Node and Its parent nodes, Give me the output whether the node is a root , leaf or branch node\nRoot - The starting node, with no parents\nLeaf - node with no child\nbranch - node with a child, that is not root.\t\nNode\tParent\n1\t2\n3\t2\n6\t8\n9\t8\n2\t5\n8\t5\n5\t\n6",
    "summary": "Find the right node",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Bitwise",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do you write an incremental update data pipeline with the following sample data\n\nemp_data : \n \nemp_id mob_no   Address                Flag\n123         7777777         Bangalore                Y\n456   8888888         Chennai                     Y\n789   9999999         Pune                        Y\n124   6666666         Hyderabad                Y\n333   9199191         Delhi                        Y\n \nemp_file :  latest data\n \nemp_id mob_no   Address\n789   9999999   Bangalore\n124   6666666   Hyderabad\n333   9199191   Bangalore\n444   7171717   Jaipur\n \n \nResult :\n \nemp_id  mob_no  Address           Flag\n123   7777777   Bangalore          Y\n456   8888888   Chennai            Y\n789   9999999   Pune               N\n124   6666666   Hyderabad          Y\n333   9199191   Delhi              N\n444   7171717   Jaipur             Y\n789   9999999   Bangalore          Y\n333   9199191   Bangalore          Y",
    "summary": "Incremental data update",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Bitwise",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Take an example and demonstrate how to read an array as a data frame.\nHow to convert an RDD into a data frame?\nHow to define custom schema?",
    "summary": "Array as a dataframe",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between repartition and coalesce?\nDifference between cache and persist?\nDifference between Wide and Narrow transformations?\nWhat is data skewness and how to handle it?\nExplain DAG?\nDifference between Broadcast variable & Accumulators?",
    "summary": "Spark theory questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Spark code to add a new column as â€œfull nameâ€ concatenating first name and last name?",
    "summary": "New column in spark",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is the difference between RDD, Dataframe & Dataset? What is the difference between Spark & MapReduce?",
    "summary": "RDD differance",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Different compression techniques in parquet? Why should we use the Parquet format? Other columnar storage formats?",
    "summary": "Compression techniques",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between rank, dense_rank, and row_number()?",
    "summary": "Rank functions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a table with 2 columns, id and value like below:\nID\nValue\n\n1\nA\n\n2\nB\n\n3\nc\n\n\nDisplay the result as below using SQL & Spark:\nA, B, C",
    "summary": "Display the alphabets",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How to find duplicates from the given table using SQL & Spark?\nHow to find the duplicate records and save them in another path in spark?",
    "summary": "Find duplicates",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given input,\nC1\nC2\n\nABC\nDEF\nPQR\n\n\n       Produce the output,\n       \nC1\nC2\n\nABC\nPQR\n\nDEF\nPQR\n\n\nGiven input,\nEmpld\nDate\nExpenses\n\n1\n21-01-2022\n50\n\n1\n22-02-2022\n100\n\n1\n04-03-2023\n120\n\n1\n04-05-2023\n300\n\n2\n03-03-2022\n100\n\n2\n04-04-2022\n300\n\n\nProduce the output,\nEmpld\nDate\nExpenses\n\n1\n21-01-2022\n50\n\n1\n22-02-2022\n150\n\n1\n04-03-2023\n270\n\n1\n04-05-2023\n570\n\n2\n03-03-2022\n100\n\n2\n04-04-2022\n400",
    "summary": "Produce the data in given pattern",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Que1 - Write an SQL query to find the month with the highest expenditure.\nQue2 - Write an SQL query to calculate the expenditure for each month\n\nItem\t  Expenditure\tDate   \nApple\t400\t    1-Jul-23\nBananas\t200\t    4-Jul-23\nCarrots\t100\t    3-Jun-23\nGuava\t50\t   20-Jun-23",
    "summary": "Highest expenditure",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPMG",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Que - Write Python or Scala program\n\ninput = \"Happy New Year\"\n\noutput 1:\nYear New Happy\n \noutput 2:\nyppah wen raey",
    "summary": "Reverse order of words",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPMG",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How many executor cores can be allocated in a 20-node Spark cluster, where each node has 16 CPU cores and 64 GB of RAM?",
    "summary": "Spark resources allocation",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPMG",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Que -Write an SQL query to find 2nd highest score in each department.\n\nStudent\tDepartment\tPoints\nRam\tECE\t9\nAjay\tECE\t9.2\nGopal\tECE\t9\nNikhil\tEEE\t8.8\nSai\tEEE\t8.5\nChaand\tEEE\t8.9\nVimal\tMech\t9\nRaju\tMech\t9\nNaveen\tMech\t9\nAditya\tCSE\t9.2\nShweta\tCSE\t9.4\nPriya\tCSE\t9.1\nManisha\tCSE\t9",
    "summary": "Highest score in each dept",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPMG",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SQL Question -\nwith source_table as(\nSELECT  'a1' as customer , 'swift' as product, 'maruti' as brand\nUNION ALL\nSELECT 'a2','vento','volkswagen'\nUNION ALL\nSELECT 'a2','polo','volkswagen'\nUNION ALL\nSELECT 'a1','hector','mg'\nUNION ALL\nSELECT 'a2','tiguan','volkswagen'\nUNION ALL\nSELECT 'a3','swift','maruti'\nUNION ALL\nSELECT 'a3','scross','maruti'\n)\nselect * from source_table\n \n\n-- normal-has bought multiple products from multiple brands\n-- loyal_customer-has bought products only from single brand\n-- Write sql query to find brand and number of loyal customers?",
    "summary": "Loyal customers",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Czech Republic",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "input - \"aaabbccdaaabdd\"\noutput - \"a3b2c2d1a3b1d2\"\nWrite a Python program to transform the input string?",
    "summary": "character count in string",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "Czech Republic",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do you handle late arriving data in your incremental loads?",
    "summary": "Late arriving records",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "Czech Republic",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Find the count of each letters in a word using SQL\nINSERT INTO sample_table (text_column) VALUES\n('this'),\n('another'),\n('text');",
    "summary": "character count",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "Czech Republic",
    "ctc": "40L-50L",
    "companyName": "MSD",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How to split the delimiter and load into a dataframe using pyspark\nName~|Age\nVirat, Kohli~|28\nAndrew, Simond~|137\nGeogre, Bush~|159\nFlintoff, David~|12\nAdam, James~|20",
    "summary": "Split the delimiter",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Czech Republic",
    "ctc": "40L-50L",
    "companyName": "MSD",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "how you can secure the s3 bucket?",
    "summary": "secure s3 bucket",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "Czech Republic",
    "ctc": "40L-50L",
    "companyName": "MSD",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Ques - Write a Python 3 program to check for duplicates in a given list of items and print only the duplicates.\narr = [10, 20, 20, 10, 10, 20, 5, 20]",
    "summary": "Duplicates in a list",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Ernst & Young",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Que-How many records will be present in the result for each of the following joins?\n\nLeft Join\nRight Join\nInner Join\nFull Outer Join\n\nColumn 1\n0\n1\n2\nNULL\n\nColumn 2\n2\n4\nNULL\nNULL",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Ernst & Young",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Que - Create a DataFrame with the following column names: order_id, order_date, customer_id, payment_status. Perform the following manipulations:\n\n1.Add a new column with the current timestamp as current_date.\n2.Drop duplicates based on the primary key (order_date, customer_id).\n3.Drop the order_id column.\n\nlist_1 = [\n    [1, \"2013-07-25\", 11599, \"CLOSED\"],\n    [2, \"2014-07-25\", 256, \"PENDING_PAYMENT\"],\n    [3, \"2013-07-25\", 11599, \"COMPLETE\"],\n    [4, \"2019-07-25\", 8827, \"CLOSED\"]\n]",
    "summary": "Order data transformations",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Ernst & Young",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Python\nlst=[1,2,4,9,1,6,7] get prime numbers from the list",
    "summary": "Prime numbers in list",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "SQL\nWrite a query to delete duplicates from table",
    "summary": "Delete duplicates",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "in redshift how do u get data from a file residing in s3",
    "summary": "Fetch data into redshift",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "1. Catalyst Optimizer and Adaptive Query Execution (AQE): - How both help in optimization",
    "summary": "Catalyst Optimizer",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Apexon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "3. How broadcast join help in optimization for skew Data",
    "summary": "Broadcast in data skewness",
    "difficulty": "Hard",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Apexon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Python Program to Check if a String is Palindrome or Not",
    "summary": "Palindrome string",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Apexon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "2. Airflow Task rerun: - does it generate same run ID and we need to give different ID",
    "summary": "Airflow task rerun",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Apexon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "sort and merge 2 list.",
    "summary": "Sort & merge",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Apexon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find the record having the latest date for each ID - to answer in both SQL and Pyspark\n +----+------------+--------+\n  | id | date       | value  |\n  +----+------------+--------+\n  | 1  | 2024-01-16 | AA     |\n  | 2  | 2023-05-17 | BB     |\n  | 1  | 2023-06-13 | AC     |\n  | 2  | 2024-03-18 | AD     |\n  +----+------------+--------+",
    "summary": "Latest ID data",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EXL Services",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "+-------------+---------+\n| Column Name | Type    |\n+-------------+---------+\n| id          | int     |\n| num         | varchar |\n+-------------+---------+\nid is the Unique key for this table.\nid is an autoincrement column.\n\nFind all numbers that appear at least three times consecutively.\n\nReturn the result table in any order.\n\nExample 1:\n\nInput: \nLogs table:\n+----+-----+\n| id | num |\n+----+-----+\n| 1  | 1   | \n| 2  | 1   |\n| 3  | 1   |\n| 4  | 2   |\n| 5  | 1   |\n| 6  | 2   |\n| 7  | 2   |\n+----+-----+\nOutput: \n+-----------------+\n| ConsecutiveNums |\n+-----------------+\n| 1               |\n+-----------------+\nExplanation: 1 is the only number that appears consecutively for at least three times.",
    "summary": "Consecutive appearance",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EXL Services",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "What is data modeling?\nWhat are Star and Snowflake schemas?\nWrite a high level data model that has many-to-many mappings?\nWhat are SCD types 0,1,2? And where do you use them?",
    "summary": "DE theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EXL Services",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain the concept of dimensional modeling in data warehouses. How does it differ from normalized modeling, and in which scenarios would you prefer one over the other? Provide examples to support your answer",
    "summary": "Dimensional Modelling",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Delloite",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are SCD?",
    "summary": "SCD",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Delloite",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a dataset of customer transactions stored as a PySpark DataFrame:\n\nCustomerID\tTransactionDate\tAmount\tCategory\n101\t2024-12-01\t500\tElectronics\n102\t2024-12-02\t150\tGroceries\n101\t2024-12-03\t300\tApparel\n102\t2024-12-04\t100\tGroceries\n103\t2024-12-05\t250\tElectronics\n\nWrite a PySpark code to calculate the total spending per customer.\nFilter the customers who have spent more than 400 in total.",
    "summary": "Customer spend transactions",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Delloite",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "About the optimization technique used in SQL and Pyspark.",
    "summary": "Optimization techniques",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Delloite",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given two tables:\n\nCustomers\n\nCustomerID\tName\tCity\n1\tAlice\tNew York\n2\tBob\tLondon\n3\tCharlie\tParis\n\nOrders\n\nOrderID\tCustomerID\tOrderAmount\tOrderDate\n101\t1\t500\t2024-11-01\n102\t2\t200\t2024-11-02\n103\t3\t300\t2024-11-03\n104\t1\t400\t2024-11-04\n\nWrite an SQL query to find:\n\nThe total order amount for each customer along with their name and city.\nCustomers who have placed more than one order and the total amount spent.",
    "summary": "Customers orders",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Delloite",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a table named Sales:\nSaleID\tProductID\tSaleAmount\tSaleDate\n1\t101\t500\t2024-10-01\n2\t102\t300\t2024-10-02\n3\t101\t400\t2024-10-03\n4\t103\t600\t2024-10-04\nAnd a table named Products:\nProductID\tProductName\tCategory\n101\tLaptop\tElectronics\n102\tPhone\tElectronics\n103\tChair\tFurniture\nWrite a query to calculate total sales for each product category.\nIdentify the product with the highest sales amount.",
    "summary": "Highest sales in product",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a list of dictionaries representing students' scores:\nWrite a Python program to calculate the average score for each student.\nFind the name of the student with the highest average score.",
    "summary": "Average score of students",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are surrogate keys, and why are they preferred over natural keys in dimensional modeling? Provide scenarios where surrogate keys can lead to better performance or simplify design.",
    "summary": "Surrogate Keys",
    "difficulty": "Hard",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You have a PySpark DataFrame of employees and their salaries:\nEmployeeID\tDepartment\tSalary\tDateHired\n1\tHR\t50000\t2020-06-15\n2\tIT\t75000\t2018-09-20\n3\tHR\t60000\t2022-03-11\n4\tIT\t72000\t2021-12-01\nWrite a PySpark code to calculate the average salary per department.\nFilter out departments where the average salary is less than 60,000.\nAdd a column indicating how many years each employee has worked in the company (based on the current date)",
    "summary": "Employee experience in dept",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are tasked with designing a data model for an e-commerce system. The requirements include:\n\nTracking customers and their addresses.\nStoring product details like categories and prices.\nRecording orders placed by customers, including timestamps and quantities.\nDraw an Entity-Relationship (ER) diagram for the data model.\nDescribe how you would normalize this model to the third normal form (3NF).",
    "summary": "E-Commerce Data Model",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Explain the concept of shuffle operations in PySpark. Why are shuffle operations costly, and how can their impact be minimized? Provide examples of transformations in PySpark that result in shuffle operations",
    "summary": "Spark shuffling",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Aidetic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a text file having a unstructured text (fixed width file), search a word ex: test and get the frequency of that word as output in entire file",
    "summary": "Frequency of word",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between lineage and DAG",
    "summary": "Lineage & DAG",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "project Architecture",
    "summary": "Project Architecture",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "val data = Seq(\n  (1, \"#\", \"\"),\n  (2, \"dd\", \"$\"),\n  (3, \"NA\", \"5000\"),\n  (4, \"John\", \"NA\"),\n  (5, \"\", \"6000\")\n)\n \nval specialChars = List(\"#\", \"$\", \"%\", \"&\", \"@\")\n\nspecial chars should be broadcasted\n\ncompare both data and special chars , if cols in data having any of the special chars replace it with null",
    "summary": "Special characters in data",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "val data = Seq(\n  (101, \"[pizza,samosa,idli]\"),\n  (102, \"[kachori,sambhar,idli]\"),\n  (103, \"[dosa,vada,pizza]\"),\n  (104, \"[samosa,idli,chai]\"),\n  (105, \"[pizza,chai,dosa]\")\n)\n\ngiven orderid and orders. i want aggregated output as number of times each item is orderd",
    "summary": "Count of items ordered",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "difference between repartition and coalesce , explain it in paint(in pc)",
    "summary": "Repartition & Coalesce",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "input : Hi Hello World\noutput : World Hello Hi\n\nsolve using python",
    "summary": "Reverse the sentance",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Coforge",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "I have deleted one partition from HDFS end , now will my metastore identify that \nmissed partition ?",
    "summary": "Missing partitions",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How you will decide the Spark Configuration properties",
    "summary": "Spark configuration properties",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Name|Age|Education|Year\nRam|28|Maths,Physics,Chemistry|2021\nRakesh|29|Biology,Physics,Chemistry|2021\nMadhu|21|Maths,Physics,Chemistry|2021\nSuman|21|Maths,Physics|2021\nRadhika|21||2021\n\nprocess the data and transform the column \"Education\". expalin the variations of explode (explode,explode_outer,poseexplode,poseexplode_outer)",
    "summary": "Explode function",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "val data = Seq(\n      (1, Seq(2, 3, 4)),\n      (2, Seq(1, 3, 4)),\n      (3, Seq(1, 2)),\n      (4, Seq(1, 2))\n    )\n\ngiven userid and friendslist. find the mutual friends between two friends",
    "summary": "Mutual friend list",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is the diff between stages and tasks ?\ndiiference between cache and persist?\ndifference between client and cluster mode?",
    "summary": "Spark UI",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "I want to change my internal table to external table , what is the command ?",
    "summary": "Hive managed table",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "two sum in scala",
    "summary": "Sum of 2 numbers in scala",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Project Architecture and related questions",
    "summary": "Project Architecture",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "given an array, push the zeros to end and print the rearranged array using scala",
    "summary": "Rearranged array",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Explain the Hive Architecture ?\nWhat is meant by METASTORE ?",
    "summary": "Hive metastore",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "There are n children standing in a line. Each child is assigned a rating value given in the integer array ratings.\n\nYou are giving candies to these children subjected to the following requirements:\n\nEach child must have at least one candy.\nChildren with a higher rating get more candies than their neighbors.\nReturn the minimum number of candies you need to have to distribute the candies to the children.\nInput: [1,2,2]\nOutput: 4",
    "summary": "Candies to children",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "write a program to check give string is palindrome or not \nstr=\" A man , a plane, a canal\"",
    "summary": "Palindrome string",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "There is a file in lambda A and how to do you move that same file to lambda B",
    "summary": "File in Lambda",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Astrosoft Technologies",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Input:\ntable\nid\tname\n1\tIndia\n2\tPakistan\n3\tBangladesh\n4\tSrilanka\n5\tAfghanistan\n\noutput:\nIndia Vs Pakistan\nPakistan Vs Bangladesh\nSrilanka Vs Afghanistan",
    "summary": "Matches between countries",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input: \nstudent_id\t\tsubject\t\tmarks\n1\t\tEnglish\t\t50\n1\t\tMaths\t\t90\n1\t\tScience\t\t70\n2\t\tEnglish\t\t60\n2\t\tMaths\t\t80\n2\t\tScience\t\t90\n\noutput:\nsubject,first_mark\nThe output should contain subject and first_mark of each subject in descending order.",
    "summary": "Top marks in each subject",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is difference betweem hive internal and external table?\nWhy Spark is faster than MP?\nWhat is repartition in Spark?",
    "summary": "Spark theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tiger Analytics",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what is Lazy evaluation in spark\nDifference between cache and persist and their syntax",
    "summary": "Lazy evaluation",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "synchrony",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given below 2 tables what are outupts for each join\nTable 1\nId\n1\nNull\nNull\n1 \n\nTable 2\nId\nNull\n1\nNull\n1 \n\ninner join \nleft outer join \nright outer \nfull",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "synchrony",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Below is the in input generate output as below which consists of no of not null values in each column \t \nCol A\tCol B\tCol C\n1\t1\t1\n2\t2\t2\n3\tNULL\t3\n4\tNULL\tNULL\n5\tNULL\tNULL\n \t \t \n \t \t \nOutput\t \t \nCol A\tCol B\tCol C\n5\t2\t3\n\nUse of union fuction",
    "summary": "Count non-null values",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "synchrony",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "you have a csv file with columns adhaar_ID , name , dob, transaction_id, trancsaction_amount, load_timestamp on hdfs path \"/another/hdfs/path/csv_adhaar\"\nadhaar_ID is like a nation id for a person living in India\nname : name of the person who did a transaction \ndob : dob of the person who did a transaction\ntransaction_id: id for each transaction done\ntransaction_amount : amount used for transaction\nload_timestamp : time when the transaction loaded into the table.\nhas context menu\n\nfind the person who did the second least transaction done last month using spark",
    "summary": "Aadhaar Transaction",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "synchrony",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "We have 3 tables A,B,C .write a sql query to fetch details from table B which are not in A,C",
    "summary": "Fetch details from tables",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "synchrony",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what is set in python",
    "summary": "Python set",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Table Name : Products\n\nColumn_Name\tType\n\nproduct_id\tint\nproduct_name\tvarchar(50)\ncategory\tvarchar(50)\n\nproduct_id is Primary Key of this table.\n\nTable Name : Sales\n\nColumn_Name\tType\n\nproduct_id\tint\nyear\tint\ntotal_sales_revenue\tDECIMAL(10, 2)\n\n\nproduct_id, year is the Primary Key of this table.\nproduct_id is the Foreign key to Products table.\n\nWrite a sql query to find the products whose total sales revenue has increased every year. Include the product_id , product_name and category in the result. Sort the result by product_id.",
    "summary": "Increase in sales revenue",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Table Name : login_details\n\nColumn Name\tType\ntimes\ttime\nstatus\tvarchar(3)\n\ntimes is the primary key column for this table and increasing in order.\n'in'\n'out'\n\n\nThis table provides login and logoff details of one user.\n\nWrite a SQL query to to reqpresent the different periods (in mins) when user was logged in.\n\n\n\n\n\ntimes\tstatus\n10:00:00\ton\n10:01:00\ton\n10:02:00\ton\n10:03:00\toff\n10:04:00\ton\n10:05:00\ton\n10:06:00\toff\n10:07:00\toff\n10:08:00\toff\n10:09:00\ton\n10:10:00\ton\n10:11:00\ton\n10:12:00\ton\n10:13:00\toff\n10:14:00\toff\n10:15:00\ton\n10:16:00\toff\n10:17:00\toff\n\n\n\nlog_on\tlog_off\tduration\n10:00:00\t10:03:00\t3\n10:04:00\t10:06:00\t2\n10:09:00\t10:13:00\t4\n10:15:00\t10:16:00\t1",
    "summary": "Users login activity",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a program to find repeated words from given list",
    "summary": "Repeated words",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a program to find the repeated word in given list.",
    "summary": "Repeated words",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find the 3-month rolling average of total revenue from purchases given a table with users, their purchase amount, and date purchased (YYYY-MM-DD). Output the year-month (YYYY-MM) and 3-month rolling average of revenue, sorted from earliest month to latest month.",
    "summary": "Rolling average of revenue",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Assume that you are working with Big Data in a project that follows the Star Schema data warehousing model. In this architecture, you have fact tables and dimension tables. There may be situations where the dimension tables are relatively small, while the fact tables are extremely large. You are tasked with preparing a report that combines both the fact and dimension tables.\n\nGiven this scenario, what strategies or methods would you use to optimize the processing speed when combining data from these tables? How would you ensure faster query performance and efficient processing in this case?",
    "summary": "Processing dimensional modelling tables",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Suppose you have two dataframes:\nThe first dataframe contains columns ID (integer) and name.\nThe second dataframe contains columns ID (string) and salary.\nIf you attempt to join these two dataframes on the ID column, what will be the behavior when the data types of ID differ (i.e., one being an integer and the other a string)? Specifically, how will SQL Server handle this join operation, and will the join succeed?",
    "summary": "Spark dataframes join",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accenture",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Implement SCD2 in Address table",
    "summary": "Implementation of SCD-2",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "American Airlines",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are the different type of SCDs? Explain with examples.",
    "summary": "Types of SCD",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "American Airlines",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You have 2 tables namely A and B. Perform left join, inner join, full outer join on both the tables and give the join results\n\nA :\n\n 1\n 2\n 1\n NULL\n\nB:\n 1\n 1\n NULL",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "American Airlines",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input:\ndata1 = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n\ndata2 = [(1, \"HR\"), (2, \"Finance\"), (4, \"IT\")]\n\ndata1_schema = [\"name\", \"dept_id\"]\n\ndata2_schema= [\"dept_id\", \"department\"]\n\nJoin these two tables and display the below output in pyspark:\n1,\"Alice\",\"HR\"\n2,\"Bob\",\"Finance\"\n4,NULL,\"IT\"",
    "summary": "Spark dataframe role display",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CAPGEMINI",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Emp - id, location, salary \nDep - depid, emp_id\n\nemp_data = [(1,'blr',10000),(2,'chn',20000),(3,'pune',5000)]\nemp_schema = ['id','location','salary']\n\ndep_data = [(10,1),(10,2),(20,3)]\ndep_schema = ['depid','empid']\nfrom depid 10 how many emp we have in Pune and total salary in pyspark?",
    "summary": "",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CAPGEMINI",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input:\ndata1 = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)] \n\ndata2 = [(1, \"HR\"), (2, \"Finance\"), (4, \"IT\")]\n\ndata1_schema = [\"name\", \"dept_id\"]\n\ndata2_schema= [\"dept_id\", \"department\"]\n\nJoin these two tables and display the below output in pyspark:\n1,\"Alice\",\"HR\"\n2,\"Bob\",\"Finance\"",
    "summary": "Spark dataframe dept display",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CAPGEMINI",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "DIfference between list and tuple",
    "summary": "List & Tuple",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CAPGEMINI",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what is ingress in k8's",
    "summary": "Ingress in K8",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Need to derive the due column from the data. There a table with columns load_id, demand,collection, demanddate,collectiondate.\nUser a demand a loan amount daily as 100. he needs to pay to bank daily basics i.e,  collection column. if the user paid on that day then due will be zero as user paid on the same day right only. If he skips the amount which he needs pay then due will be one, one next day user requested 100 and paid 100 then due will one because the previous day he didnt paid right so due will be one. attaching the details below.\nInput - demand,collection, demanddate,collectiondate \n   100 100 2025010120250101 \n  100 100 20250102 20250102 \n  100 0 20250103 20250103 \n  100 100 20250104 20250104 \n  100 0 20250105 20250105 \n  100 300 20250106 20250106 . \noutput - \ndemand,collection, demanddate,collectiondate, due \n   100 100 2025010120250101 0\n  100 100 20250102 20250102 0\n  100 0 20250103 20250103 1\n  100 100 20250104 20250104 1\n  100 0 20250105 20250105 2\n  100 300 20250106 20250106 0. \n\nSummary of the question if user paid the demanded amount the due will be 0, if user skips amount the due will be 1, due will increase as days will increase if wont pay, user skips for two days then due will 2(5 row). if user paid all the due amount then due will become zero(last row).",
    "summary": "User due loan amount",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what are clustering key in snowfalke",
    "summary": "clustering key in snowflake",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "find the second highest salary from the table",
    "summary": "Second highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "how can we reduced snowflake cost. and explain snowflake architecture",
    "summary": "Reduce snowflake cost",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "how the data is stored in snowflake",
    "summary": "Data storage in redshift",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Kirana Capital",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a pyspark program to find the word count of a given txt file\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import split, explode, regexp, regexp_extract, col\n\nspark=SparkSession.builder.appName(\"count\").master(\"local\").getOrCreate()\ndf=spark.read.text(\"D:\\python_final\\word_count_sample.txt\")\ndf.show()\ndf_split=df.withColumn(\"result\",split(\"value\",\" \")).drop(\"value\")\ndf_explode=df_split.withColumn(\"words\",explode(df_split[\"result\"]))\ndf_explode.show()\ndf_explode.groupby(\"words\").count().orderBy(col(\"count\").desc()).show()",
    "summary": "Word count in txt file",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "delloite",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "I have a data with 2 columns id and count we have to display name column based on the value of an count column and count column should be like 0 to count\ninput:+---+-----+\n| id|count|\n+---+-----+\n|  1|    2|\n|  2|    3|\n|  3|    4|\n+---+-----+\noutput:\n+---+---+\n| id|seq|\n+---+---+\n|  1|  0|\n|  1|  1|\n|  2|  0|\n|  2|  1|\n|  2|  2|\n|  3|  0|\n|  3|  1|\n|  3|  2|\n|  3|  3|\nsolution:\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import expr, explode\nfrom pyspark.sql.functions import concat_ws,collect_list\n\n# Initialize Spark session\nspark = SparkSession.builder.appName(\"RepeatIDs\").getOrCreate()\n\n# Create a DataFrame with 'id' and 'count' columns\ndata = [(1, 2), (2, 3), (3, 4)]\ncolumns = [\"id\", \"count\"]\n\ndf = spark.createDataFrame(data, columns)\ndf.show()\n\n# Create a sequence from 0 to count-1, then expand it using explode\ndf.createOrReplaceTempView(\"repeat\")\ndf_expanded = (df.withColumn(\n    \"seq\", explode(expr(\"sequence(0, count - 1)\")))).drop(\"count\")\ndf_expanded.show()\n#df_expanded.selectExpr(\"id\", \"explode(seq) as repeated\").show() # Explode sequence to rows\n\n# Show the result\n\ndf_expanded.show(truncate=False)\nspark.sql(\"select id,explode(sequence(0,count-1)) from repeat\").show()",
    "summary": "Spark dataframes sequencing",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "delloite",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Suppose you're reading the data from csv files but it has multiple delimiters how can you read the data from csv in pyspark \ndf_final_csv=df_csv2.withColumn(\"value\",expr(\"regexp_replace(value,'[^a-zA-Z0-9]',',')\"))",
    "summary": "Handling multiple delimiters",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "delloite",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given a dataframe containing the customer transactions - \nschema\n----------------------------------------\ncustomer_id                     String\ntransaction_timestamp    Timestamp\n----------------------------------------\nFind the distinct number of customers who ordered at least once every month in the past year",
    "summary": "Distinct orders from customers",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "- Explain AQE, what are the benefits of AQE. How does AQE help (Dynamically Coalescing, Dynamically handling Partition Skew, Dynamically Switching Join Strategies).\n- Have used UDFs in Spark? What is your opinion about the same? What is it's impact on performance? How about vectorized UDFs?\n- Databricks and different types of compute in Databricks.\n- What are the different join strategies in Spark?\n- Have you used constraints in Databricks?\n- What is Delta format? What is the advantage it gives? Explain vacuum command?\n- What is small file problem? How can you solve it?\n- What are the different security options in BigQuery?\n- What are the different types of storage in BigQuery?\n- What is row level and column level security in BigQuery?",
    "summary": "DE theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given a file, read the contents of the file and then create a word counter function using python file handler(Not using Pyspark or pandas, use the python basic file handler here)",
    "summary": "Python file handler",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You have a Spark pipeline that is working properly in Dev environment, you finished your development and testing and you are releasing it for UAT in the UAT environment, but there you notice that the Spark job is taking too long to complete. How would you approach this situation? What could be the case?\nHint - the data in Dev and UAT can vary significantly in size and pattern.",
    "summary": "Data testing in UAT",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantium Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given multiple data sources -> DBs like Oracle/Postgres, Flat files, APIs, SFTP etc. Design an end to end architecture in GCP using different services such that it can fulfill both batch and real time data needs for the end users. The solution should have an efficient way to store and process huge amount of data and provisions for end users to query the data and do dashboarding.",
    "summary": "Design architecture handling batch&real data",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantium Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Follow up to the design question\n1) What are some high level optimization techniques that you would use in your data warehouse? Explain them.\n2) How do you make sure that the entire workflow is automated?\n3) Will you choose different ingestion tools for different data sources? What is your opinion?\n4) How about making sure that the data is secured and only the right people have access to it?",
    "summary": "Data design questions",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantium Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "1) SQL Challenge: \n\nYou have been given two tables: \"orders\" and \"customers.\" The \"orders\" table contains order information, including the order ID, customer ID, order date, and order amount. The \"customers\" table contains customer information, including the customer ID, customer name, and customer city. \n\nWrite a SQL query to retrieve the top 5 customers (based on the total order amount) from the city of \"London\" who have placed at least 2 orders. The result should include the customer name, city, and total order amount.",
    "summary": "Top customers in london",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "BLOOMBERG",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "3) PySpark Challenge: \n\nYou have been given a dataset containing information about user activity on a website. The dataset consists of the following columns: \"user_id\" (integer), \"timestamp\" (timestamp), \"page_visited\" (string). Your task is to write a PySpark program to perform the following operations: \n\nRead the dataset into a PySpark DataFrame. \n\nCalculate the total count of page visits for each user. \n\nFind the user with the highest number of page visits. \n\nCalculate the average number of page visits per user. \n\nWrite the results to a new CSV file named \"user_activity_summary.csv\" with the following columns: \"user_id,\" \"total_page_visits,\" \"average_page_visits\". \n\nFor this challenge, you can assume that the dataset is in a CSV file format and the column delimiter is a comma.",
    "summary": "Page visitors in pyspark",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "BLOOMBERG",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "2) Python Challenge: \n\nYou are given a CSV file named \"data.csv\" that contains customer information. The file has the following columns: \" name,\" \"address,\" \"email,\" and \"phone.\" Your task is to write a Python program to read the CSV file and perform the following operations: \n\nValidate the email addresses and phone numbers to ensure they are in the correct format. \n\nWrite the cleaned data to a new CSV file named \"cleaned_data.csv\" with the same column structure. \n\nFor email validation, assume a valid email address has the format: username@domain.com. For phone number validation, assume a valid phone number has 10 digits (no other characters). \n\nNote: You can use the built-in csv module in Python to read and write CSV files. \n\nRemove any leading or trailing whitespace from the values in the \"customer_name,\" \"email,\" and \"phone_number\" columns.",
    "summary": "CSV data processing",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "BLOOMBERG",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "We will have the employee table empid and mgrid \nwirte the Pyspark query for to find the manager level  and employee count reporting to them",
    "summary": "Employees reporting to manager",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Natwest",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "explain you project architecture and your role and your teams roles.",
    "summary": "Project Architecture",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Natwest",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "we have 2 tables tableA and tableB with column B how many rows output for all 4 join types\nA   B\n1    1\n1    1\n      1\n      1",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Natwest",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Which is frequent used file format and explain different file formats and their usage in your project.",
    "summary": "File formats in project",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Natwest",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "we have customer table with orderdate, orderid, customerid,qty the output should be the with customerid and days_taken_to_2nd_order_from1st and  days_taken_to_3rd_order_from1st",
    "summary": "Days taken for next order",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Mcafee",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "how are you migrating the databricks workflows.",
    "summary": "Databricks migration",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Mcafee",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "we have susbscriptions table with subid,tansactionid,duration in the out the duration should be concated as a single value for the respective subid.",
    "summary": "combine subscriptions",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Mcafee",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "In below mention PySpark Operation, how many will be executed in driver node vs worker node\nFunction Invocation\nFilter \nMap\nResult and Formatting\nreduceByKeys\nSave file to AWS S3 Bucket",
    "summary": "Execution tasks",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Axa XL",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the below dataset using pyspark, find the employee with 2nd highest salary\nemp_id|name|salary|dept_id\n1|Donald|2600|Sales\n2|Douglas|2600|Sales\n3|Jennifer|4400|IT\n4|Michael|13000|IT\n5|Pat|6000|HR\n6|Susan|6500|IT\n7|Hermann|10000|IT\n8|Shelley|12008|Sales\n9|William|8300|HR\n10|Steven|24000|IT\n11|Neena|17000|HR\n12|Lex|17000|IT\n13|Alexander|9000|Sales\n14|Bruce|6000|HR\n15|David|4800|IT",
    "summary": "Employee second highest salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Axa XL",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the below dataset, read the file in PySpark and transform the dataset such that is has 3 columns :- first_name, last_name, age. \nName ~| Age\nArjun, Kapoor ~| 25\nAlia, Bhatt ~| 24\nDeepika, Padukone ~| 26\nRanbir, Kapoor ~|26",
    "summary": "Actors data transformation",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Axa XL",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given below 2 tables using SQL, find all the number of records from left, right and inner join output\ntable 1 :\n1\n2\n1\n2\n4\nNULL\n\ntable 2:\n1\n1\n2\n2\n3\nNULL",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Axa XL",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to retrieve number of customers who have made a purchase in last 30 days but did not purchase anything in the previous 30 days",
    "summary": "No purchase made",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "McKinsey",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the below PySpark script, how many time will the script actually execute?\n\ndf1 = spark.read.parquet()\ndf2 = df1.filter()\ndf3 = df2.groupBy().agg()\ndf4 = df3.filter()\n\ndf4.write.parquet()\ndf4.count()",
    "summary": "Count script execution",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "McKinsey",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How many types of transformations are present in PySpark. State with an example for each",
    "summary": "Pyspark transformations",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "McKinsey",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Describe a ETL pipeline you deployed on AWS service. What all transformation did you apply? How did you perform delta detection? How did you orchestrate your pipeline etc",
    "summary": "Delta data detection",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "McKinsey",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "For every 6 cups of coffee I buy, I would get 7th cup for free. Write a program in python that prints total number of cups I will get given 'n' cups as input",
    "summary": "Totals cups of coffee",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "McKinsey",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "2)\nhow to delete logical duplicates from table?\ncol1 Col2\nA     B\nB     A\nC     D\nD     C\nE     F\n\noutput:\ncol1 col2\nA     B\nC     D\nE     F",
    "summary": "Delete logical duplicates",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Q1). mother asked you to bring 2 types of Fruits from Market. You went to Market. Fruit Vendor has the following Fruits\nFruit\n-----\nApple\nBananas\nOranges\n \nYou called your Mom to asked which 2 sets of fruits needs to brought ?\noutput:\nApple, Oranges\nApple Bananas\nBananas Oranges\nWrite a sql query for the above output",
    "summary": "Fruits combination",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "3)Python\nrotate the array by K elements [1,2,3,4,5,6] k=3\noutput: [4,5,6,1,2,3]",
    "summary": "Rotate an array",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a Code to load nested JSON with dynamic schema in table using pyspark",
    "summary": "Load nested JSON",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KOTAK BANK",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Search target Element in Rotated Sorted Array and return its index(solve it in log(n) complexity)\narr=[4,5,6,7,1,2,3] target=1\noutput: 4",
    "summary": "Index of sorted array",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KOTAK BANK",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "python\n1) flatten the nested list [1,2,3,[4,5,[1,2,3]]] \noutput: [1,2,3,4,5,1,2,3]",
    "summary": "Flatten nested list",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KOTAK BANK",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Data Question : My data is coming from 5 Sources A,B,C,D & E, I have to maintain whole data in one single table how i manage that?",
    "summary": "Data from 5 sources",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Bloomberg UK PLC",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "2. Create a schema in spark, take any example & include various datatype",
    "summary": "Schema in spark",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Bloomberg UK PLC",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "3. Python Question:\n\nInput : [1,1,2,2,2,3,3,3,3,1]\n\nResult: (1,2),(2,3),(3,4),(1,1)",
    "summary": "Frequency of numbers",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Bloomberg UK PLC",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1. Python Question :\nInput : Put values of list A in list B as shown below\n\nA = [1,2,3,4,5,5,1,1,3,6]\n\nB = [{},[],()]\n\nOutput : [{1,1,1},[2],(3,3),{4},[5,5],(6)]",
    "summary": "List values formatting",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Bloomberg UK PLC",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Context: You are tasked with analyzing clickstream data to understand user behavior on a platform. The platform's clickstream data is stored in the following table:\n\nTable: Clickstream\n\n+--------------+-----------+-------------------------------------+\n| Column Name  | Data Type | Description                         |\n+--------------+-----------+-------------------------------------+\n| UserId       | INT       | Unique identifier for each user.    |\n| ClickTime    | DATETIME  | Timestamp of a user's click event.  |\n+--------------+-----------+-------------------------------------+\n\n+--------+---------------------+\n| UserId | ClickTime           |\n+--------+---------------------+\n| 1      | 2025-01-20 10:00:00 |\n| 1      | 2025-01-20 10:15:00 |\n| 1      | 2025-01-20 11:00:00 |\n| 2      | 2025-01-20 09:00:00 |\n| 2      | 2025-01-20 09:45:00 |\n+--------+---------------------+\n\n\nRequirements:\n\nA new session starts if either:\nIt is the first click for the user.\nThe time difference between two consecutive clicks exceeds 30 minutes.\n\nFor each user, determine using Pyspark:\n1. The total number of sessions (TotalSessions).\n2. The total time spent across all sessions (TotalTimeSpent), calculated as the sum of the durations of all their sessions.",
    "summary": "User clicks in platform",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Publicis Sapient",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "1.How can you control the number of partitions in a DataFrame or RDD?\n2.What are the implications of too many or too few partitions on performance?\n3.How does PySpark handle data shuffling, and how can you minimize its impact?\n4.Why are DataFrames considered more efficient than RDDs in PySpark?",
    "summary": "Spark Theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Publicis Sapient",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Scenario: You are tasked with analyzing the performance using SQL of 4 teams participating in the Asia Cup. The match results are stored in the following table:\n\n+-------------+---------+--------------------------------------+\n| Column Name | Data Type | Description                        |\n+-------------+---------+--------------------------------------+\n| MatchId     | INT      | Unique identifier for each match.   |\n| TeamA       | VARCHAR  | Name of the first team.             |\n| TeamB       | VARCHAR  | Name of the second team.            |\n| RunsA       | INT      | Runs scored by Team A.              |\n| RunsB       | INT      | Runs scored by Team B.              |\n| MatchDate   | DATE     | Date on which the match occurred.   |\n+-------------+---------+--------------------------------------+\nSample Data:\n\n\n+---------+--------+--------+-------+-------+------------+\n| MatchId | TeamA  | TeamB  | RunsA | RunsB | MatchDate  |\n+---------+--------+--------+-------+-------+------------+\n| 1       | India  | Pakistan | 250   | 200   | 2025-01-10 |\n| 2       | India  | Sri Lanka| 300   | 320   | 2025-01-12 |\n| 3       | Pakistan | Bangladesh | 280   | 275   | 2025-01-14 |\n| 4       | Sri Lanka | Bangladesh | 310   | 290   | 2025-01-16 |\n| 5       | India  | Bangladesh | 270   | 260   | 2025-01-18 |\n+---------+--------+--------+-------+-------+------------+\nRequirements:\n\n1.Determine the total matches played by each team.\n2. Calculate the number of matches won by each team (a team wins if it scores more runs than its opponent).\n3. Rank the teams based on their total wins.",
    "summary": "Asia cup matches",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Publicis Sapient",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "1. How would you handle incremental data loading in a data warehouse to minimize processing time and system overhead?\n2. What are the advantages and disadvantages of using a star schema compared to a snowflake schema?",
    "summary": "DE theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Tiger Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Question 1:\n\nScenario: A company tracks employee attendance data in the following table:\n\n+-------------+----------+---------------------+\n| Column Name | Data Type | Description |\n+-------------+----------+---------------------+\n| EmployeeId | INT | Unique employee ID. |\n| CheckIn | DATETIME | Employee check-in time. |\n| CheckOut | DATETIME | Employee check-out time. |\n| Date | DATE | Date of attendance. |\n+-------------+----------+---------------------+\nSample Data:\n\n+------------+---------------------+---------------------+------------+\n| EmployeeId | CheckIn | CheckOut | Date |\n+------------+---------------------+---------------------+------------+\n| 1 | 2025-01-01 09:00:00 | 2025-01-01 17:00:00 | 2025-01-01 |\n| 1 | 2025-01-02 09:30:00 | 2025-01-02 16:45:00 | 2025-01-02 |\n| 2 | 2025-01-01 10:00:00 | 2025-01-01 18:00:00 | 2025-01-01 |\n| 3 | 2025-01-01 08:45:00 | 2025-01-01 17:15:00 | 2025-01-01 |\n| 3 | 2025-01-02 08:50:00 | 2025-01-02 17:10:00 | 2025-01-02 |\n+------------+---------------------+---------------------+------------+\nRequirements:\n\n1.Calculate the total working hours for each employee for each day using SQL.\n2.Find the employee with the maximum total working hours across all dates using SQL.",
    "summary": "Employee attendance tracker",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Tiger Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Scenario: You are analyzing the sales performance of an online store using the following table:\n\n+--------------+-----------+-----------------------------+\n| Column Name | Data Type | Description |\n+--------------+-----------+-----------------------------+\n| OrderId | INT | Unique identifier for orders. |\n| CustomerId | INT | Unique identifier for customers. |\n| OrderDate | DATE | Date when the order was placed. |\n| Amount | DECIMAL | Total amount of the order. |\n| Region | VARCHAR | Region where the order was placed. |\n+--------------+-----------+-----------------------------+\nSample Data:\n\n+---------+------------+------------+--------+--------+\n| OrderId | CustomerId | OrderDate | Amount | Region |\n+---------+------------+------------+--------+--------+\n| 1 | 101 | 2025-01-01 | 200.50 | North |\n| 2 | 102 | 2025-01-01 | 150.00 | South |\n| 3 | 103 | 2025-01-02 | 300.75 | East |\n| 4 | 101 | 2025-01-03 | 120.00 | North |\n| 5 | 102 | 2025-01-03 | 180.50 | West |\n+---------+------------+------------+--------+--------+\nRequirements: Write an SQL query to generate below insights\n\n1.Calculate the total sales amount per region.\n2.Find the top 3 customers based on their total purchase amount.",
    "summary": "Online sales performance",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Tiger Analytics",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Customer data is stored in table which contains a column mobile number. Write an sql query or pyspark code to check if the mobile number is in given format(xxxx-xxx-xxx)",
    "summary": "Mobile number format",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tekion",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "How to decide the number of workers in AWS Glue",
    "summary": "AWS Glue workers",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tekion",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "What is the maximum runtime for lambda",
    "summary": "Lambda runtime",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tekion",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Write a python code to identify the common the prefix in the list of the string provided.\nexample: [\"flask\",\"flank\",\"flower\"]  ans: \"fl\"",
    "summary": "Common prefix",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tekion",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "What is a decorator? explain it with an example.",
    "summary": "Decorator in python",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Recro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is repartition and coalesce? when will you use them?",
    "summary": "Repartition in spark",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Recro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the FruitHarvest table, write a SQL query to retrieve the record ID, farm ID, harvest date, harvest quantity, and the average harvest quantity over the current harvest and the three preceding harvests for the same farm. The result should be ordered by farm ID and harvest date.\n \nTable:\nrecord_id\tfarm_id\tharvest_date\tharvest_quantity\n1\t701\t2023-03-01\t120\n2\t701\t2023-03-10\t150\n3\t701\t2023-03-20\t180\n4\t701\t2023-03-30\t200\n5\t702\t2023-04-01\t220\n6\t702\t2023-04-10\t250\n7\t702\t2023-04-20\t270\n8\t702\t2023-04-30\t300",
    "summary": "Fruit farm harvesting",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Recro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a code to trigger the pipeline in adf.",
    "summary": "Trigger pipeline in ADF",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Recro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to get an output as an output table.\n\nt1 Country\n----------\nIndia\t\nSrilanka\nAustralia\nPakistan\n \nt2 Country\n--------\nIndia\nSrilanka\nAustralia\nPakistan\n \nOutput\n=====\nCountry1   Country2\t\t\n-------------------\nIndia      Srilanka\nIndia      Australia\nIndia      Pakistan\nSrilanka   Australia\nSrilanka   Pakistan\nAustralia  Pakistan",
    "summary": "Countries combination",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to get the output as an output table.\nInput table\ncommunication_code\tevent_type\ncom1\t              Sent\ncom2\t              Open\ncom3\t              Sent\ncom1\t              Bounced\ncom3\t              Bounced\ncom2\t              Sent\ncom2\t              Sent\ncom1\t              Bounced\n\n\nOutput\n======= \ncommunication_code\tSent\tOpen\tBounced\ncom1\t             1\t     0\t      2\ncom2\t             2\t     1\t      0\ncom3\t             1\t     0\t      1",
    "summary": "communication codes",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to get the max salary from the each department.",
    "summary": "Max salary from dept",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do you handle a schema change in Azure Data Factory?",
    "summary": "Schema change in ADF",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Explain your current project in brief.",
    "summary": "Current project",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Affine",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q1. How do you monitor your data pipeline?",
    "summary": "Data pipelines monitoring",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "aptlytech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do you handle the Sev2 incident in your project?",
    "summary": "Severity-2 issue",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "aptlytech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query to get the second highest salary.",
    "summary": "Second highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "aptlytech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What would be the output of the below code?\n\nresult = joinedDF.select('cust_name').filter(joinedDF.lob = 4 AND datediff(current_date(),joinedDF.booking_date)=30).show()",
    "summary": "Dataframe output",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "aptlytech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Could you share the challenges you encountered while implementing the project?",
    "summary": "Project challanges",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "aptlytech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is Autoloader in Databricks?",
    "summary": "Autoloader in databricks",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain how memory allocation will be done in Python and Spark?",
    "summary": "Memory allocation in spark",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You are given a dataset with date, product_id, and sales_qty. Some rows contain null values for sales_qty. You are asked to forward-fill the missing values with the last non-null value per product_id in time order\n\nNote: Do not use any in-built functions\n \ndata = [\n    (\"2024-01-01\", 101, 10),\n    (\"2024-01-02\", 101, None),\n    (\"2024-01-03\", 101, 15),\n    (\"2024-01-01\", 102, 20),\n    (\"2024-01-02\", 102, None),\n    (\"2024-01-03\", 102, None),\n\n]\n \ncolumns = [\"date\", \"product_id\", \"sales_qty\"]",
    "summary": "Forward filling values",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You are given a dataset containing customer information. Some fields have NULL values. Write code to replace NULL values with default values. For the age column, replace NULL with the average age, and for the city column, replace NULL with 'Unknown'.\n\ndata = [\n    (1, \"John\", 25, \"New York\"),\n    (2, \"Sarah\", None, \"San Francisco\"),\n    (3, \"Michael\", 40, None),\n    (4, \"Jessica\", None, \"Los Angeles\"),\n    (5, \"David\", 35, \"Seattle\")\n]\n \ncolumns = [\"customer_id\", \"name\", \"age\", \"city\"]",
    "summary": "Customer info dataframe",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "What is Catalyst optimizer in Spark?",
    "summary": "catalyst optimizer",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "You are working for an e-commerce platform that tracks customer transactions. You are given a dataset containing customer_id, purchase_date, product_id, and quantity. The platform wants to analyze repeat purchases. Your task is to identify the customers who bought the same product on more than one occasion within a 30-day window.\n \ndata = [\n    (101, \"2024-01-01\", 201, 2),\n    (101, \"2024-01-15\", 201, 3),\n    (102, \"2024-01-02\", 202, 1),\n    (103, \"2024-02-05\", 203, 5),\n    (103, \"2024-02-10\", 203, 2),\n    (101, \"2024-03-01\", 201, 1),\n]\n \ncolumns = [\"customer_id\", \"purchase_date\", \"product_id\", \"quantity\"]",
    "summary": "Repeat order customers",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neudesic",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Wordcupnum\tCountry\n1\tWest Indies\n2\tWest Indies\n3\tIndia\n4\tAustralia\n5\tPakistan\n6\tSri Lanka\n7\tAustralia\n8\tAustralia\n9\tAustralia\n10\tIndia\n11\tAustralia\n12\tEngland\n13\tAustralia\nFind consecutive winner using pyspark",
    "summary": "Consecutive winter",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "EY",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "01. Let's assume through SFTP File came and resided on S3 and duplicate came . We need both old and new file how do you keep both ?\n02. How do you schedule the Glue job ? \n03. Why event based approach is feasible for running Glue Job ? If Yes how ? If No why ?\n04. Glue architecture ?\n05. spark-submit\n06. What transformation have you done ?\n07. There are junk values in the file/data frame that is read how do you find it and rectify it\n08. How do you deploy?\n09.  Let's say there are 100s of projects and you need to give required access for projects for required AWS Service how would you automate ?\nAny reason to choose Terraform over CloudFormation when IAC on AWS can be done using AWS CloudFormation.\n10. Difference between SparkSession, SparkContext and Hivecontext\n11. Why service will you use for Notifying users , Keep the notification in queue the delete the notification ?\n12. There are different ways to connect to on-premise Database to AWS i.e. DMS,JDBC,GLUE-Connection when all does the same work\nwhat's the need for different ways?\n13. Let assume you have 100 files yesterday in S3 Bucket , Today you see 70 files . 30 Files are delete . You have not implemented CloudWatch logging \nHow do you verify the delete files ?\n14. Let assume there is a long running job, redshift team reports that data is not refreshing ? How do you handle this issue ? What may be the cause ?\n15. How do you know data skewness happened and how do you conclude the issue reason ?",
    "summary": "DE scenarios",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "EY",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Customer table:\n+------------+-------------+\n| customer_id | customer_name |\n+------------+-------------+\n| 101 | Alice |\n| 102 | Bob |\n| 103 | Charlie |\n+------------+-------------+\n\nOrders table:\n+-------------+------------+--------------+-------------+-------------+\n| order_id | sale_date | order_cost | customer_id | seller_id |\n+-----------+----------+------------+-----------+-----------+\n| 1 | 2020-03-01 | 1500 | 101 | 1 |\n| 2 | 2020-05-25 | 2400 | 102 | 2 |\n| 3 | 2019-05-25 | 800 | 101 | 3 |\n| 4 | 2020-09-13 | 1000 | 103 | 2 |\n| 5 | 2019-02-11 | 700 | 101 | 2 |\n+-----------+----------+------------+-----------+-----------\n+Seller table:\n+-----------+-----------+\n| seller_id | seller_name |\n+-----------+-----------+\n| 1 | Daniel |\n| 2 | Elizabeth |\n| 3 | Frank\n\nWrite an SQL query to report the names of all sellers who did not make any sales in 2020",
    "summary": "No sales made",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "EY",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "PySpark Question :\n Give a Employee Table .Find the top 2 salaries for each department.\n\nemp_id\temp_name\tdepartment_id\tsalary\n1\tAlice\t1\t100000\n2\tBob\t        1\t90000\n3\tCharlie\t1\t95000\n4\tDavid\t2\t120000\n5\tEve\t        2\t110000\n6\tFrank\t2\t115000\n7\tGrace\t3\t85000\n8\tHarry\t3\t80000\n9\tIvy\t        3\t88000",
    "summary": "Top salaries in dept",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CGI",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "01. Introduce about Yourself , Latest Project.\n02. Why AWS Glue ? Why not AWS EMR ?\n03. Have you used Dynamic Data frame in your project ?\n04. How is Schema Evolution Handled .\n05. Have you worked on NoSQL / AWS DynamoDB ? What is Pagination\n06. Have you used AWS Step Function , have you used Cloud Formation or Terraform ? \n07. How do you delete 10 years data without writing Code\n08. Have you worked or do you know on CI/CD ?\n   how do you do unit testing ?\n09. Have you used any APIs.\n10. what is Vertical and Horizontal Scaling?\n11. Do you use OOPS concepts in Project like polymorphism, inheritance ?\n12. How do you find duplicates count in the data frame.\n13. Suppose there is a Data frame , need to change the datatype of one column to String Type from Int Type how do you do ?\n14. What is Executors ? Spark Architecture .\n15. What is Logical and Physical Plan ?\n16. Difference between drop, truncate, purge in spark.",
    "summary": "DE scenario questions",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CGI",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Python question :\n      input_list=[0,1,2,3,4,5,6,7,8,9]\n      output_list=[{0,8},{1,7},{2,6},{3,5}]",
    "summary": "Python list pattern",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "CGI",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Difference between star schema and snowflake schema",
    "summary": "Snowflake schema",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "JoulestoWatts",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Get  customer who are placing highest orders from below tables\n\ncustomers\n+----+----------+\n| ID | Name     |\n+----+----------+\n| 1  | John     |\n| 2  | Alice    |\n| 3  | Bob      |\n| 4  | Sarah    |\n+----+----------+\n\norders\n+-----+------------+\n| ID  | CustomerID |\n+-----+------------+\n| 101 | 1          |\n| 102 | 2          |\n| 103 | 3          |\n| 104 | 1          |\n| 105 | 3          |\n| 106 | 4          |\n| 107 | 2          |\n| 108 | 3          |\n+-----+------------+",
    "summary": "Highest orders placed",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "JoulestoWatts",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Get the expected output from below python input:\ninput = [1,2,{2,3},[5,4]]\noutput: [1,2,3,4,5]",
    "summary": "Python input lists",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "JoulestoWatts",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "read 2 csv files from S3.\n1st CSV - remove DOB NULL records\n2nd CSV - remove duplicate from SSN \nJoin both and write to other s3 bucket",
    "summary": "CSV files operations",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a query to fetch details of employees whose EmpLname ends with an alphabet â€˜Aâ€™ and contains five alphabets",
    "summary": "Employee name",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write an SQL query to create an empty table with the same structure as some other table.",
    "summary": "SQL table structure",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "LTIMindtree",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "id,dept,date,sales\n1,electronics,01-09-22,2000\n2,dresses,03-09-22,10000\n1,fitness,07-09-2022,23000\n2, groceries,10-09-2022,14000\n3,sweets,03-09-2022,5000\n3,restaurant,04-09-2022,6000\n1,shoes,12-09-2022,8000\n2,computers,15-09-2022,6700\n\ncreate dataframe using the CSV data guven above. assume it is in a CSV file\n    find the details of customer id 1\n    find the total of all customers between the start date and end date\n    find which customer gives maximum sales\n    find on which day each customer gets the highest sales",
    "summary": "Customers orders in CSV",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "dbs",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "val list1 = List(1, 2, 3)\nval list2 = List(\"a\",\"b\", \"c\")\n\no/p: List(a1,b2, c3)",
    "summary": "Python lists",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "dbs",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "you are running 2 processes at same time which are running into load data at same hive table into the same partition at sametime. What do you think will happen. Both the jobs will run fine? or one of the fail or one of them succeed.",
    "summary": "2 parallel spark jobs",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "dbs",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "you are taking 2 parameters. one is jar name and 2nd parameter you are reading is hdfs destinition location. You want to read the jar and inside the jar there are lot .proprties files. After reading the jar you have extract all the .properties files from jar and place them in the destinition hdfs localton which is provided a second parameter. Can you please write a shell script for that.",
    "summary": "Hdfs jar script",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KPI Partners",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a dataframe to handle below json schema\n{\n\"group\" : {},\n\"lang\" : [ \n    [ 1, \"scala\", \"functional\" ], \n    [ 2, \"java\",\"object\" ], \n    [ 3, \"py\",\"interpreted\" ]\n]\n}",
    "summary": "Pyspark json schema",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KPI Partners",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "What is highest temparature in each year ?\n\nI/p\nCITY,YEAR,TEMP\nBanglore,2020,35\nChennai,2020,31\nMumbai,2020,32\nChennai,2021,26\nHyderabad,2021,32\nMumbai,2021,31\n\nO/p\nCITY,YEAR,TEMP\nBanglore,2020,35\nHyderabad,2021,32",
    "summary": "Highest temperature",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KPI Partners",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Have you worked on any encryption decryption functionalities. What kind of data you deal with?",
    "summary": "Data encryption",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KPI Partners",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Remove the special characters in name column using data frame\nid,name,roll_num\n1,\"\"\"\"Narayana\"\"\"\",100\n2,\"NarayanaNew\"\",200",
    "summary": "Special characters removal",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "KPI Partners",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a function to locate the left insertion point for a specified value in sorted order.",
    "summary": "Left insertion point",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Creditsafe Technology",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write SQL query to find running total from a table",
    "summary": "Running total",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Creditsafe Technology",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write Python/Pyspark code to do following:\n1. Read 2 tables from oracle db\n2. join tables\n3. Convert to csv\n4. Convert to json\n5. Upload to s3",
    "summary": "Pyspark file operations",
    "difficulty": "Hard",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Creditsafe Technology",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "We are loading the output from below query to a file everyday. This process is taking more time, How to optimize it?\n\nQuery:\nselcct * from tabl1\nunion\nselcct * from tabl2\nunion\nselcct * from tabl3\nunion\nselcct * from tabl4",
    "summary": "File loading process",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Creditsafe Technology",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain different techniques to fill null values? Write code for it in Pyspark/Python",
    "summary": "Fill null values",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Creditsafe Technology",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given an array of integers and a value, determine if there are any two integers in the array whose sum is equal to the given value. Return true if the sum exists and return false if it does not.\n\nConsider this array and the target sums:\n\nPython solution \ndef find_sum_of_two(A, val):\n  found_values = set()\n  for a in A:\n    if val - a in found_values:\n      return True\n\n    found_values.add(a)\n    \n  return False\n\nv = [5, 7, 1, 2, 8, 4, 3]\ntest = [3, 20, 1, 2, 7]\n\nfor i in range(len(test)):\n  output = find_sum_of_two(v, test[i])\n  print(\"find_sum_of_two(v, \" + str(test[i]) + \") = \" + str(output))",
    "summary": "Sum of integers in array",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Door Dash",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "List the Products Ordered in a Period (this question directly pickup from leet code \n\n+------------------+---------+\n| Column Name      | Type    |\n+------------------+---------+\n| product_id       | int     |\n| product_name     | varchar |\n| product_category | varchar |\n+------------------+---------+\nproduct_id is the primary key (column with unique values) for this table.\nThis table contains data about the company's products.\n \n\nTable: Orders\n\n+---------------+---------+\n| Column Name   | Type    |\n+---------------+---------+\n| product_id    | int     |\n| order_date    | date    |\n| unit          | int     |\n+---------------+---------+\nThis table may have duplicate rows.\nproduct_id is a foreign key (reference column) to the Products table.\nunit is the number of products ordered in order_date.\n \n\nWrite a solution to get the names of products that have at least 100 units ordered in February 2020 and their amount.\n\nReturn the result table in any order.\n\nThe result format is in the following example.",
    "summary": "Fetch name of the products",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Door Dash",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Average Delivery Time per Restaurant\nAs an analyst at DoorDash, you are asked to measure the performance of restaurant partners. One important measure is the average delivery time associated with each restaurant. Assume that we calculate the delivery time by the difference between the order time and delivery completion time. Can you write a SQL query to find the average delivery time for each restaurant?\n\nPlease consider the following tables:\n\norders Example Input:\norder_id\torder_time\tdelivery_time\trestaurant_id\tcustomer_id\n0001\t08/25/2021 18:00:00\t08/25/2021 18:40:00\t100\t123\n0002\t08/25/2021 19:00:00\t08/25/2021 19:30:00\t200\t265\n0003\t08/25/2021 20:00:00\t08/25/2021 20:40:00\t200\t362\n0004\t08/25/2021 21:00:00\t08/25/2021 21:35:00\t300\t192\n0005\t08/25/2021 22:00:00\t08/25/2021 22:45:00\t100\t981\nExample Output:\nrestaurant_id\tavg_delivery_time_in_minutes\n100\t42.5\n200\t35.0\n300\t35.0",
    "summary": "Average delivery time per restaurant",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "Door Dash",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write sql query to get Nth highest salary",
    "summary": "Nth highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find the customer who purchased items from all category using SQL & pysaprk\nCustomer - name , cat_id\nCategory - cat_id, pur_id\npurchase - purchase_id,cat_id",
    "summary": "All items purchase",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How would you group products with different prices using PySpark?\nAnswer : using collect_list",
    "summary": "Products grouping",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Give the count of rows in result for Inner join, left join, right join, full outer join\n for the following tables\n\nTable 1 \nid -1,1,1,1,1\n\nTable2\nId - 1,1,1,1,1,1",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "remove duplicates from array \n\nGiven an integer array nums sorted in non-decreasing order, remove some duplicates in-place such that each unique element appears at most twice. The relative order of the elements should be kept the same.\n\nSince it is impossible to change the length of the array in some languages, you must instead have the result be placed in the first part of the array nums. More formally, if there are k elements after removing the duplicates, then the first k elements of nums should hold the final result. It does not matter what you leave beyond the first k elements.\n\nReturn k after placing the final result in the first k slots of nums.",
    "summary": "Relative order of elements",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "QRT",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "find the top 3 salaries from each department and order them by descending order.",
    "summary": "Top 3 salaries",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "QRT",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given an array nums with n objects colored red, white, or blue, sort them in-place so that objects of the same color are adjacent, with the colors in the order red, white, and blue.\n\nWe will use the integers 0, 1, and 2 to represent the color red, white, and blue, respectively.",
    "summary": "Coloured objects",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "50L-60L",
    "companyName": "QRT",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "2.SQL: We have 4 tables, employees, jobtitle, salary, address\n1. write a query to display the first_name, last_name, jobtitle and salary of each employee. (JOINS)\n2. write a query to display the first_name, last_name of employees whose address is not present in the address table. (LEFT JOIN)\n3. write a query to display the first_name, last_name and the most recent address of each employee. (Window functions and CTE)",
    "summary": "Employee table functions",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "20L-30L",
    "companyName": "Avanade",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Imagine you are doing a migration project and you have ingested 1000 records from a table and after doing data cleaning and transformations as per business requirement, the target has 800 records in it. Now, the QA teams comes back to you and says that as per their testing, there should be 900 records, not 800. How would you handle such scenario?",
    "summary": "Data migration scenario",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "Canada",
    "ctc": "20L-30L",
    "companyName": "Avanade",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Python: We have a list with file names of some csv files that we want to load to a target. We also have a base input path and base output path. Write a function, to read all the csv files and load them as parquet files in the output path.",
    "summary": "CSV file loading",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "20L-30L",
    "companyName": "Avanade",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find Cumulative Sum\n\nid sales\n1   20\n2   40\n3    50\n4    90",
    "summary": "Cumulative sum",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "CitiusTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a pyspark code\n1. read csv file\n2. Do deduplication\n3. Create new column age_category on given condition\n4. write the final dataframe as a delta table and partition it on age_caegory.",
    "summary": "Pysarpk CSV operations",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "CitiusTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "UNION and UNION ALL, HAVING and WHERE difference.",
    "summary": "SQL functions difference",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "CitiusTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Which LTS you used?, Then Explain OPTIMIZE and Z ORDER",
    "summary": "LTS usage",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "CitiusTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is Dimensional modelling, Types of Schemas and what is durable key and surrogate key",
    "summary": "Dimensional modelling",
    "difficulty": "Hard",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "CitiusTech",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Achieve expected output from Input table.\n\nInput table\n------------\nEmp Floor\n\na    1\n\nb    4\n\nc    5\n\na    3\n\nd    2\n\nd    6\n\nOutput table\n---------------\nEmp Floor\n\na    Multiple\n\nb    4\n\nc    5\n\nd   Multiple",
    "summary": "Employees in floors",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPI Partners",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Left join output of these 2 tables\n\nTable 1\t\nID \tcol1\n1\tA\n2\tC\n3\tB\nnull\tnull\nnull\tnull\n\t\n\n\nTable 2\t\nID \tcol1\n2\tE\n2\tF\n3\tF\nnull\tnull",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPI Partners",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a program to generate below output using python.\n\nExpected output:\n------------------\n1\n121\n12321\n1234321\n123454321",
    "summary": "Generate sequence pattern",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "KPI Partners",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a pyspark code to get highest 3 salaries in each department",
    "summary": "Highest salary in dept",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "EXL Services",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Difference between SCD type2 and type3.Show with an example",
    "summary": "SCD Types",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "EXL Services",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a sql query for below type of table to get third highest salary in each department also consider department having less than 3 employees\nempid empname depid salary",
    "summary": "Dept with less employees",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "EXL Services",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "For below tables write the count of outputs for every join\nTable A\n1\n1\n \nTable B\n1\n1\n1",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "EXL Services",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write SQL query to get output as below \ncountry\nind\naus\nnz\n\n output\nind vs aus\nind vs nz\naus vs nz",
    "summary": "Cricket matches",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "EXL Services",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do we remove duplicates from file using Datastage",
    "summary": "Datastage duplicates",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "TCS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Load 10 input files to 10 target tables at a time which has different metadata",
    "summary": "Load files with different metadata",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "TCS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "I have 10 Million records in source and target 5 million records were populated then \njob was aborted again i go and compile and run the job when that time job will run from\n 5million 1 record to remain records how?",
    "summary": "Records population in table",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "TCS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do we handle large dataset & tune the performance in datastage",
    "summary": "Peformance tuning in datastage",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "TCS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "How do we reverse a string in datastage",
    "summary": "String reverse",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "TCS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "4. Write pySpark code for below scanrio\nInput:\n('James','Java'),\n('James','Python'),\n('James','Python'),\n('Anna','PHP'),\n('Anna','Javascript'),\n('Maria','Java'),\n('Maria','C++'),\n('James','Scala'),\n('Anna','PHP'),\n('Anna','HTML')\nOutput:\njames, [java,pyhton,scala]",
    "summary": "Pyspark languages list",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Clairvoyant",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "10. SQL window funtions\n11. AWS EMR and lambda funtions\n12. How to build realtime ETL pipeline in AWS",
    "summary": "DE theory questions",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Clairvoyant",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "5.Write python code to print all the substrings from given input string\n6.Write SQL Query for Number of employees falling in each band\n \nInput file:\nempid,empsal\n102, 20000,\n103, 30000,\n140, 56999,\n\nOutput:\nbandwidth, number of employee in that bandwidth\n 0-10000 , 0\n 10001-20000, 1\n 20001-30000,1",
    "summary": "Employees salary bandwidth",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Clairvoyant",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1. Spark Architecture.\n2. Diff b/w RDD and Df\n3. Spark performance tuning steps & realtime scenario'scenario",
    "summary": "Spark questions",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Clairvoyant",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1.Given a file which is big (~ 1Gb) and a small file small file (~ 100 mb)  you have to read both files and find out the pincodes with top 5 population.\n \nBig file                                                                                 Small file\ncity_id  city_name   population  country                           city id.        pincode\n1           Hyderabad    1000000       India                             1                500032",
    "summary": "Pincode with highest population",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Oracle",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "2.SQL Query for match Fixers:\nId, TeamName\n1, India\n2, Australia\n3, England\n4, New Zealand\n\n\nIndia vs Australia\nIndia vs England\nIndia vs New Zealand\nAustralia vs England\nAustralia vs New Zealand\nEngland vs New Zealand\n\n3. Write Pyspark code to mask Email column",
    "summary": "Match fixers",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Oracle",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "12. How to upload TB file to S3. How to invoke lambda functions\n\n13. Data bricks realted basic questions\n\n14. Job scheduling pattern in Airflow\n\n15. SCD2 implementation in Pyspark\n\n16. How to handle ETL flow and monitor , fail over scenarios\n\n17. Count of null values in each column\n\n18. Merge to soreted arrays into single array",
    "summary": "DE theory",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Oracle",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1. Hive paritioning\n\n2. Tuple, list and dicts and performance related questions\n\n3. Schema evaluation in Spark",
    "summary": "DE theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Oracle",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Write a python code to find Largest Sum Contiguous Subarray\n\n[1, 2, 3, 4, 5] => 15\n[4, -6, 2, 5] => 7",
    "summary": "Subarray largest sum",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Concentrix",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1. How to delete partitions in Hive\n2. How to use broad cast in Spark SQL\n3. Data modeling..difference between snowflake and star schema\n4. challenge to migrate pyspark app from python 2 to 3\n5. Employee & department tables SQL queries",
    "summary": "Project questions",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Concentrix",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1. Python lambda implementation, dict and list comprehension\n2. Number of default partions while loading 200GB single csv in Spark\n3. cache and persist and Out of memory scenario questions\n4. AQE in spark\n5. How to improve performance of spark jobs",
    "summary": "DE theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Concentrix",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "5. SQL queries\n\nInput:\nEmpId,EmpName,ManagerId\n1,Ramesh,2\n2,Suresh,3\n3,Mahesh,4\n4,CEO,null\n5,Mallesh,3\n\nOutput:\nEmpName,Level_1_MgrName,Level_2_MgrName,Level_3_MgrName\nRamesh,Suresh,Mahesh,CEO\nSuresh,Mahesh,CEO,null\nMahesh,CEO,null,null\nMallesh,Mahesh,CEO,null\nCEO,null,null,null",
    "summary": "Manager levels of employees",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Concentrix",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "\"Give me the count of records for inner join,left join,right join and full outer join for the below 2 tables.\n\ntable A\nid1\n1\n1\n2\n2\n \ntable B\n \nid1\n1\n1\n1\n4\n2\"",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "LandMark Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"Write a sql query to transform rows to cols witout using pivots given data :\n\nInput:\nemp_id\tsalary_type\tval\n1\tsalary\t10000\n1\tbonus\t5000\n1\thike_prc\t10\n2\tsalary\t15000\n2\tbonus\t7000\n2\thike_prc\t8\n3\tsalary\t12000\n3\tbonus\t6000\n3\thike_prc\t7\n\nOutput:\nemp_id\tsalary\tbonus\thike_prc\n1\t10000\t5000\t10\n2\t15000\t7000\t8\n3\t12000\t6000\t7",
    "summary": "Salary hike percent",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "LandMark Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"Solve this without using window functions:\n\n\ndate\t\tsale \n2023-10-03\t10   \n2023-10-04\t20     \n2023-10-05\t60   \n2023-10-07\t50   \n2023-10-08\t10   \n\n\ndate\t\tsale\t%var from prev day\n2023-10-03\t10\t\t\n2023-10-04\t20\t\t100\"",
    "summary": "Sales variance",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "LandMark Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"WAP python program:\ninput: l1=[1,2,[3,4,[5,6,7],8],9,10]\noutput: l2=[1,2,3,4,5,6,7,8,9,10]\"",
    "summary": "Python print list",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "LandMark Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "3. Find number of instances where signal 1 is greater than 50 and signal 2 is greater than 55\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\nimport random\n \n# Initialize Spark session\nspark = SparkSession.builder.appName(\"Sample DataFrame\").getOrCreate()\n \n# Define the schema\nschema = StructType([\n    StructField(\"Vehicle Number\", StringType(), True),\n    StructField(\"Time Stamp\", IntegerType(), True),\n    StructField(\"Signal Name\", StringType(), True),\n    StructField(\"Value\", IntegerType(), True)\n])\n \n# Create sample data\ndata = []\nvehicles = [\"V1\", \"V2\"]\nsignals = [\"Signal1\", \"Signal2\", \"Signal3\"]\ntimestamps = [1, 2, 3, 4, 5]\n \nfor vehicle in vehicles:\n    for ts in timestamps:\n        for signal in signals:\n            value = random.randint(1, 100)  # Random value between 1 and 100\n            data.append((vehicle, ts, signal, value))\n \n# Create DataFrame\ndf = spark.createDataFrame(data, schema=schema)\ndf.display()\n\nAns: Explained the logic how to solve using pivot logic but failed to address how to handle for large dataset.",
    "summary": "Vehicle stops near signal",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "2. In the table defined below show the number of occurances for which the speed between two consecutive time sample changed more than 700 rpm\n\ndf = spark.createDataFrame([(1,2000),(2,2100),(3,4000),(5,4500),(6,60),(7,90),(8,9000)],schema=['time','engrpm'])\n\nAns: \nfrom pyspark.sql import Window as W\nfrom pyspark.sql import functions as F\ndf.withColumn('Speed_lag', F.lag('engrpm').over(W.orderBy(F.col('time').desc()))).withColumn('diff', F.abs(F.col('engrpm') - F.col('speed_lag'))).where('diff >= 700').display()",
    "summary": "Speed lag difference",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. Find maximum call duration using pyspark API (Both incoming and outgoing should be calculated).\n\ndf = spark.createDataFrame([(\"Coimbatore\",\"Thirunelveli\",20),(\"Madurai\",\"Thirunelveli\",15),(\"Madurai\",\"Thootukudi\",150),(\"Madurai\",\"Coimbatore\",15),(\"Coimbatore\",\"Chennai\",15),(\"Tiruchi\",\"Coimbatore\",15)],schema=['Origin','Destination','CallDuration'])\n\nAns:\nfrom pyspark.sql import functions as F\ndf.groupBy('Origin').agg(F.sum('CallDuration').alias('MaxDuration')).sort(F.col('MaxDuration').desc()).display()",
    "summary": "Maximum call duration",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Deloitte",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. You want to view the second highest revenue generated by the product each year. Use Pyspark API to solve the problem\n\nNote: No dataset was given in the interview. Instructed to code in notepad++. \nProviding a sample dataset below for your reference. \ndata = [\n    # Year 2022 - 10 products\n    (2022, \"Electronics\", 12000),\n    (2022, \"Electronics\", 10000),\n    (2022, \"Electronics\", 15000),\n    (2022, \"Clothing 5000),\n    (2022, \"Clothing\", 7000),\n    (2022, \"Clothing\", 6500),\n    (2022, \"Grocery\", 3000),\n    (2022, \"Grocery\", 2500),\n    (2022, \"Grocery\", 3200),\n    (2022, \"Grocery\", 2800),\n\n    # Year 2023 - 10 products\n    (2023, \"Electronics\", 14000),\n    (2023, \"Electronics\", 11000),\n    (2023, \"Electronics\", 16000),\n    (2023, \"Clothing\", 6000),\n    (2023, \"Clothing\", 8000),\n    (2023, \"Clothing\", 7000),\n    (2023, \"Grocery\", 3500),\n    (2023, \"Grocery\", 3000),\n    (2023, \"Grocery\", 3800),\n    (2023, \"Grocery\", 3300),\n\n    # Year 2024 - 10 products\n    (2024, \"Electronics\", 16000),\n    (2024, \"Electronics\", 13000),\n    (2024, \"Electronics\", 17000),\n    (2024, \"Clothing\", 7000),\n    (2024, \"Clothing\", 9000),\n    (2024, \"Clothing\", 7500),\n    (2024, \"Grocery\", 4000),\n    (2024, \"Grocery\", 3500),\n    (2024, \"Grocery, 4200),\n    (2024, \"Grocery\", 3800),\n]\n\n# Define schema\ncolumns = [\"year\", \"product\", \"revenue\"]\n\n# Create DataFrame\ndf = spark.createDataFrame(data, columns)\n\nHint: Need to use Dense_Rank() functio",
    "summary": "Second highest revenue",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. What are the optimization Techniques you used in your project? How would you identify skew join and steps you will take to solve it?\n\nAns: Cache, Checkpointing, Repartition to distribute data even across partition. Avoiding UDFs if spark has inbuilt operation to handle it. To check data skewness - use spark ui metrics",
    "summary": "Spark optimization techniques",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is broadcasting and when to use it. How it helps to complete the task run faster. Write the code to broadcast smaller dataframe to larger dataframe.\n\nAns: sales_df.join(broadcast(products_df), \"product_id\", \"inner\")",
    "summary": "Broadcast join",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Read a JSON file containing salary data, compare two columns (salary1 and salary2), and generate a result based on whether salary1 is greater than salary2. Store the result in a Parquet file.\n[\n  {\"id\": 1, \"salary1\": 50000, \"salary2\": 45000},\n  {\"id\": 2, \"salary1\": 60000, \"salary2\": 70000},\n  {\"id\": 3, \"salary1\": 80000, \"salary2\": 75000},\n  {\"id\": 4, \"salary1\": 45000, \"salary2\": 45000}\n]",
    "summary": "Salary differences",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Koantek",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"filters all the elements in the list that has â€˜interviewâ€™ in the element .[\"\"pyspark\"\", \n  \"\"interview\"\", \n  \"\"questions\"\", \n  \"\"at\"\", \n  \"\"interviewbit\"\"]\"",
    "summary": "Filtering Interview",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Koantek",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"From the given tables, give the count for left,right,inner and full join\n\nTable-A\n-----\n1\n1\n0\nnull\n \n\nTable-B\n------\n1\n0\nnull\nnull\"",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Koantek",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "\"Read the csv file using Spark API and convert each row into mutiple rows based on hobbies.\n==================\n \nName, Age,hobbies\n \nChandran,28,singing-dancing-reading\n \n\nOutput\n \n===================\n \nName, Age,hobbies\n \nChandran,28,singing\n \nChandran,28,dancing\n \nChandran,28,reading\"",
    "summary": "User hobbies",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Koantek",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "# Find the products whose total sales has increased every year\ndata = [\n    (1, 2020, 100), (1, 2021, 200), (1, 2022, 300),\n    (2, 2020, 150), (2, 2021, 120), (2, 2022, 200),\n    (3, 2020, 500), (3, 2021, 700), (3, 2022, 900)\n]\ncolumns = [\"product_id\", \"year\", \"sales\"]",
    "summary": "Sales increase every year",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "30L-40L",
    "companyName": "PWC",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "# arr=[25,30,11,15,10]\nperform bubble sort on above array.",
    "summary": "Bubble sort",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "30L-40L",
    "companyName": "PWC",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what is the role of Receivers in spark streaming ?\nHow can you optimize a spark streaming Job ?\nHow to convert Parquet format to avro format.  ?",
    "summary": "DE theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "Canada",
    "ctc": "30L-40L",
    "companyName": "PWC",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "There are 10 files with different schema in a S3 bucket. design a Generic and robust system to read files and ingest into table with schema enforcement.",
    "summary": "Design file system",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "Canada",
    "ctc": "30L-40L",
    "companyName": "PWC",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a recursive cte to generate numbers  from 1to 100.",
    "summary": "Recursive CTE",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "left join vs right join example",
    "summary": "SQL Joins",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write an example dag which schedules everyday at 3am",
    "summary": "DAG Schedule",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "write a dataflow pipeline using apachebeam which takes csv file as input,transforms and loads into bigquery .",
    "summary": "Apache beam pipeline",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what is capacitor in bigquery",
    "summary": "Capacitor in bigquery",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "map vs flatmap",
    "summary": "Map Vs Flatmap",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "explain bigquery architecture",
    "summary": "Bigquery architecture",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "anagram code in python",
    "summary": "Anagram python",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Random trees pvt ltd",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1..Given are ropes of different lengths, the task is to connect these ropes into one rope with minimum cost, such that the cost to connect two ropes is equal to the sum of their lengths.\nExamples:\nInput: arr[] = {5,4,3,2,1} ,Â \nOutput: 33\nExplanation:You are given multiple ropes of different lengths, and the goal is to connect all of them into one single rope. However, there's a cost involved each time you connect two ropes, and the cost is equal to the combined length of those two ropes. Your task is to find a way to connect all the ropes while minimizing the total cost.\nStep-by-Step Explanation:\nInitial array: {5, 4, 3, 2, 1}\n1. Combine the two smallest ropes (1 and 2):\n    * Cost = 1 + 2 = 3\n    * Updated array after combining: {5, 4, 3, 3}\n2. Combine the two smallest ropes (3 and 3):\n    * Cost = 3 + 3 = 6\n    * Updated array after combining: {5, 4, 6}\n3. Combine the two smallest ropes (4 and 5):\n    * Cost = 4 + 5 = 9\n    * Updated array after combining: {6, 9}\n4. Combine the remaining ropes (6 and 9):\n    * Cost = 6 + 9 = 15\n    * Updated array: {15}\nNow we have combined all the ropes into one, so the total cost is the sum of all the individual combination costs:\nTotal cost = 3 + 6 + 9 + 15 = 33\n\nExamples 2:\nInput: arr[] = {4,3,2,6} ,Â \nOutput: 29",
    "summary": "connect the ropes",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Sigmoid",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How to get 3rd highest salary using PySpark & SQL code?",
    "summary": "3rd highest salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Sigmoid",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "2.Question: Find the Minimum Number of Platforms Required\nProblem Statement:Given the arrival and departure times of trains at a railway station, find the minimum number of platforms required so that no train has to wait.\nExample:\nInput:Â \narrival = [900, 940, 950, 1100, 1500, 1800]\ndeparture = [910, 1200, 1120, 1130, 1900, 2000]\n\nOutput: 3.\nExample 2\nInput:arrival = [1000, 1015, 1025, 1030, 1100]departure = [1035, 1045, 1040, 1055, 1130]",
    "summary": "Train platforms",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Sigmoid",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Data Column -> id , name , age, department, salary \n\nFind the top 3 departments with the highest total salary.",
    "summary": "Highest salary departments",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "40L-50L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are the different storage options available to data engineers in Azure?\nWhat is Linked Services in ADF?\nWhat are ways to ingest data from on-premise storage to Azure cloud?\nHow do you maintain version of data ?",
    "summary": "DE theory",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "Canada",
    "ctc": "40L-50L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How do you read a large file like 1 TB and what optimization techniques can be applied. ?",
    "summary": "Read 1TB file",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "Canada",
    "ctc": "40L-50L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "String- 'abbcdaacb'\n\nQuestion - Remove adjacent duplicates.",
    "summary": "Remove adjacent duplicates",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "Canada",
    "ctc": "40L-50L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "DSA: Next Greater Element for every element in a given Array\n\nInput: arr[] = [1, 3, 2, 4]\nOutput: [3, 4, 4, -1]\nExplanation: The next larger element to 1 is 3, 3 is 4, 2 is 4 and for 4, since it doesnâ€™t exist, it is -1.\n\n\nInput: arr[] = [6, 8, 0, 1, 3]\nOutput: [8, -1, 1, 3, -1]\nExplanation: The next larger element to 6 is 8, for 8 there is no larger elements hence it is -1, for 0 it is 1 , for 1 it is 3 and then for 3 there is no larger element on right and hence -1.",
    "summary": "Next greater element",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Sigmoid",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "DSA: Search in a Sorted and Rotated Array\n\nQues: Given a sorted and rotated array arr[] of n distinct elements, the task is to find the index of the given key in the array. If the key is not present in the array, return -1.\nExamples: Input: arr[] = [5, 6, 7, 8, 9, 10, 1, 2, 3], key = 3\nOutput: 8\nExplanation: 3 is present at index 8 in arr[].\n\n\nInput: arr[] = [3, 5, 1, 2], key = 6\nOutput: -1\nExplanation: 6 is not present in arr[].",
    "summary": "Index of the key",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Sigmoid",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain how to traverse a tree in DSA and what is leaf node ,root node and balanced tree means",
    "summary": "Traverse a tree",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Sigmoid",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Explain Spark Architecture and what will be the cluster configuration if we have a 100 GB file",
    "summary": "Cluster configuration 100GB file",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EMIDS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "SQL Query for match Fixers:\nId, TeamName\n1, India\n2, Australia\n3, England\n4, New Zealand\n\nThe output should look like this \nIndia vs Australia\nIndia vs England\nIndia vs New Zealand\nAustralia vs England\nAustralia vs New Zealand\nEngland vs New Zealand",
    "summary": "Match Fixers",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EMIDS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "data = [\n    (\"U1\", \"2024-12-30 10:00:00\", \"LOGIN\"),\n    (\"U1\", \"2024-12-30 10:05:00\", \"BROWSE\"),\n    (\"U1\", \"2024-12-30 10:20:00\", \"LOGOUT\"),\n    (\"U2\", \"2024-12-30 11:00:00\", \"LOGIN\"),\n    (\"U2\", \"2024-12-30 11:15:00\", \"BROWSE\"),\n    (\"U2\", \"2024-12-30 11:30:00\", \"LOGOUT\"),  # Duplicate entry\n    (None, \"2024-12-30 12:00:00\", \"LOGIN\"),   # Missing user_id\n    (\"U3\", None, \"LOGOUT\")                    # Missing timestamp\n]\n\nQues: Determine the top 3 most frequent activity types for each user_id\nWrite above query in both SQL and PySpark",
    "summary": "Frequent activity types",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "EMIDS",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "sample_data = [{'name': \"SBI\", 'id': 100}, {'name': \"HDFC/D/B\", 'id': 200}, {'name': \"UBI'S\", 'id': 300}]\nexpected output = SBI_100,HDFC/D/B_200,UBI'S_300",
    "summary": "Python pattern print",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input: 235|0|0|1|99999|1|20230101|0 \nExpected output: 235|0|0|ONE|99999|PERFECT|20230101|0\nWrite a pyspark code to solve this.",
    "summary": "Pyspark pattern print",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input:\nName\tAddress\tEmail\tFloor\tResources\nA\tBangalore\tA@gmail.com  1\tCPU\nA\tBangalore\tA1@gmail.com 1\tCPU\nA\tBangalore\tA2@gmail.com 2\tDESKTOP\nB\tBangalore\tB@gmail.com  2\tDESKTOP\nB\tBangalore\tB1@gmail.com 2\tDESKTOP\nB\tBangalore\tB2@gmail.com 1\tMONITOR\n\nExpected Output:\nName\tTotal_visits\tmost_visited_floor\tresources_used\nA\t           3\t                        1\t                 CPU,DESKTOP\nB\t           3\t                        2\t                 DESKTOP,MONITOR",
    "summary": "Most visited building floor",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Zeta",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Question-1:- SQL\nWrite a query to find higest salary of each department.",
    "summary": "Highest salary in dept",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "ProArch",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Qestion-2:- SQL\n\nPROBLEM STATEMENT: In this problem, we are given information about Categories of Chocolates and their Prices. For each category we need to find top 2 sub categories based on it's price:\n\nIf for a category we have a single subcategory then we've to check it's price. \nif the price is >50 then we need to include in o/p\n \n SCRIPTS :\n create table category(\nCategory varchar(50),\nSubCategory varchar(50),\nPrice varchar(50) );\n\ninsert into category values('Chips', 'Bingo', 10), \n('Chips', 'Lays', 40),\n('Chips', 'Kurkure', 60), \n('Choclate', 'Dairy Milk', 120), \n('Choclate', 'Five Star', 40),\n('Choclate', 'Perk', 25), \n('Choclate', 'Munch', 5), \n('Biscuits', 'Oreo', 120)",
    "summary": "Top chocoloates sub-category",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "ProArch",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Tell me difference between GROUPBY & HAVING verbally and explain through code also.",
    "summary": "GroupBy function",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "ProArch",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "generate an end_date if end date is not availble make it as 31-12-9999\nemp_name country joining_date --emp\nA         IND     12-12-2020\nB         IND     12-12-2021\nA         PAK     12-12-2022\nB         SL      12-12-2024\nA         SL      12-12-2025\n\nexpected output:\nemp_name country joining_date end_date  \nA         IND     12-12-2020  12-12-2022\nB         IND     12-12-2021  12-12-2024\nA         PAK     12-12-2022  12-12-2025\nB         SL      12-12-2024  31-12-9999\nA         SL      12-12-2025  31-12-9999\n\nsolution:\ndf=spark.createDataFrame(data,[\"empid\",\"country\",\"joining_date\"])\ndf.show()\nwind=Window.partitionBy(\"empid\").orderBy(\"joining_date\")\n(df.withColumn(\"end_date\",lead(\"joining_date\").over(wind))\n .withColumn(\"end_date\",when(col(\"end_date\").isNull(),\"12-31-9999\").otherwise(col(\"end_date\"))).show())\ndf.createOrReplaceTempView(\"emp\")\nspark.sql(\"select empid,country,joining_date,coalesce(lead(joining_date) over(partition by empid order by joining_date) ,'12-31-9999')as end_date from emp\").show()",
    "summary": "Employee tenture",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "capgemini",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "write an sql query to delete the duplicates from the table\ndelete from cars\nwhere ctid in ( select max(ctid)\n                from cars\n                group by model, brand\n                having count(1) > 1);",
    "summary": "Duplicate deletion",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "capgemini",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "write a pyspark program to create match fixtures for the given df.\nsolution:\nfrom turtledemo.sorting_animate import partition\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col,hash\n\n# Create a DataFrame with the teams\nspark = SparkSession.builder.appName(\"null\").master(\"local\").getOrCreate()\n\nteams = [(\"India\",), (\"Pakistan\",), (\"Sri Lanka\",), (\"Bangladesh\",), (\"Australia\",), (\"New Zealand\",)]\nschema = [\"Team\"]  # Schema should be a list of column names\n\ndf_teams = spark.createDataFrame(teams, schema)\ndf_teams1=spark.createDataFrame(teams, [\"Team1\"])\n\ndf_join=df_teams.crossJoin(df_teams1).filter(col(\"Team\") <  col(\"Team1\"))\n\ndf_join.show(100)\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, row_number\nfrom pyspark.sql.window import Window\n\n# Create a Spark session\nspark = SparkSession.builder.appName(\"match_fixtures\").master(\"local\").getOrCreate()\n\n# List of teams\nteams = [(\"India\",), (\"Pakistan\",), (\"Sri Lanka\",), (\"Bangladesh\",), (\"Australia\",), (\"New Zealand\",)]\nschema = [\"Team\"]\n\n# Create DataFrames for teams\ndf_teams = spark.createDataFrame(teams, schema)\ndf_teams1 = spark.createDataFrame(teams, [\"Team1\"])\n\n# Assign row numbers to teams\nwindowSpec = Window.orderBy(\"Team\")\nwindowSpec1 = Window.orderBy(\"Team1\")# Row number will be based on alphabetical order of Team\n\ndf_teams_with_row = df_teams.withColumn(\"row_num\", row_number().over(windowSpec))\ndf_teams1_with_row = df_teams1.withColumn(\"row_num\", row_number().over(windowSpec1))\n\n# Perform a self-join based on the row numbers where Team's row number is less than Team1's\ndf_join1 = df_teams_with_row.join(df_teams1_with_row, df_teams_with_row[\"row_num\"] < df_teams1_with_row[\"row_num\"])\n\n# Select relevant columns and show results\ndf_join1.select(\"Team\",\"Team1\").orderBy(\"Team\").show()\n\ndf_join_left=df_teams.join(df_teams1,df_teams[\"Team\"]!=df_teams1[\"Team1\"],how=\"left\")\ndf_final=(df_join_left.withColumn(\"Hash\",hash(\"Team\")+hash(\"Team1\"))\n .select(\"*\",row_number().over(windowSpec.partitionBy(\"hash\")).alias(\"rank\")).filter(col(\"rank\")==1))\ndf_final.select(\"Team\",\"Team1\").orderBy(\"Team\").show()",
    "summary": "Pyspark match fixes",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "capgemini",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Problem Summary: Stock-Out Detection at FMCG Plants\n\nAn FMCG company tracks daily stock levels at multiple plants and daily sales at stores. Stores get replenished in real-time from assigned plants, but if a plant runs out of stock, replenishment fails, leading to a stock-out issue.\n\nThe goal is to identify which plants, products, and dates experienced stock-outs (i.e., when stock quantity reached zero at the plant level). This helps predict future stock shortages and optimize inventory management.\n\nKey Data Involved\n\t1.\tDaily Stock Data â€“ Tracks stock levels at the plant level.\n\t2.\tDaily Sales Data â€“ Tracks sales at the store level.\n\t3.\tPlant-Store Mapping â€“ Links stores to their respective plants.",
    "summary": "Design stock-out detection",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accordion",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "The classic two sum problem\nGiven an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n\nYou may assume that each input would have exactly one solution, and you may not use the same element twice.\n\nYou can return the answer in any order.",
    "summary": "Sum of 2 numbers",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accordion",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Questions on Medallion Architecture and Types of Schema star and snowflake schema",
    "summary": "Medallion architecture",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accordion",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "was there a scenario where you had to deploy a solution but then roll it back due to some issues or something? Was that some of the experience that you have had?",
    "summary": "Deployment roll back",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Accordion",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Show patient_id, diagnosis from admissions. Find patients admitted multiple times for the same diagnosis.\n\nPatient table \n------------------\npatient_id\tINT\nfirst_name\tTEXT\nlast_name\tTEXT\ngender\t    CHAR(1) \nbirth_date\tDATE\ncity\t    TEXT\nprovince_id\tCHAR(2) \nallergies\tTEXT\nheight\t    INT\nweight\t    INT\n\n \nadmissions table \n-----------------\npatient_id\t        INT\nadmission_date\t    DATE\ndischarge_date\t    DATE\ndiagnosis\t        TEXT \nattending_doctor_id\tINT",
    "summary": "Patients admissions",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Persistent",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Show the provinces (province_name) that has more patients identified as 'M' than 'F'.\n\nPatient table \n------------------\npatient_id\tINT\nfirst_name\tTEXT\nlast_name\tTEXT\ngender\t    CHAR(1) \nbirth_date\tDATE\ncity\t    TEXT\nprovince_id\tCHAR(2) \nallergies\tTEXT\nheight\t    INT\nweight\t    INT\n\n\nprovince table\n----------------\nprovince_id\t    CHAR(2)\nprovince_name\tTEXT",
    "summary": "Provinces of patients",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Persistent",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "If 1st Jan 2025 is on Monday then find what day it is on 26th Jan 2025. Write Solution in Pyspark.",
    "summary": "Republic day pysarpk",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Persistent",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Identify the origin and destination of each customer who traveled in the flight\n\n+-------+---------+---------+-----------+\n|cust_id|flight_id|   origin|destination|\n+-------+---------+---------+-----------+\n|      1|  Flight1|    Delhi|  Hyderabad|\n|      1|  Flight2|Hyderabad|      Kochi|\n|      1|  Flight3|    Kochi|  Mangalore|\n|      2|  Flight1|   Mumbai|    Ayodhya|\n|      2|  Flight2|  Ayodhya|  Gorakhpur|\n+-------+---------+---------+-----------+\n\nWrite solution in SQL and PySpark both.",
    "summary": "Customer traveled locations",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Publicis Sapient",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input : aaabbcccd\nOutput : a3b2c3d1\n\nWrite logic in python.",
    "summary": "String manpuilation",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Publicis Sapient",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "If 1st jan and 2nd jan 2025 is on weekend i.e. Saturday and Sunday then calculate all weekend in month of jan 2025 in pyspark.",
    "summary": "Calendar days calculation",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Publicis Sapient",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SQL: Given a table emp_details, produce a result which tells you all the direct and indirect  reportees of emp_id = 1\n\nInput:\nemp_id | name     | mgr_id\n1             CJ           NULL\n2             EB           1\n3             MB          1\n4             SW          2\n5             YT           NULL\n6             IS            5\n7             DA          4\n\nOutput:\n\nemp_id | name | mgr_id\n2      EB    1\n3      MB    1\n4      SW    2\n7       DA    4",
    "summary": "Employee direct reportees",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Python:\ninput = \"Hello World World is so beautifull if you say Hello\"\n \nFind the no.of occurences of each word and store it a dictionary.\n\nOutput: {'Hello': 2, 'World': 2, 'is': 1, 'so': 1, 'beautifull': 1, 'if': 1, 'you': 1, 'say': 1}",
    "summary": "occurrences of words",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Spark Theory Questions: \n1.Explain your any one previous project in detail.\n2. Explain Spark Architecture in Detail.\n3. Performance optimizations you have done in your project.\n4. Explain about Broadcast variable.\n5. Pyspark:\n\n\"/path/to/csv\"\n\"/path/to/json\"\n\"/path/to/parquet\"\n \nUnion the final dataframe out of above 3 files using pyspark",
    "summary": "pyspark dataframe",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Infosys",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "ADF Project Questions:\n1. What are all the sources you have connected to in ADF?\n2. What are different types of triggers in ADF and explain them.\n3. Have you worked on any incremental dataload? If so explain them.\n4. What type of operations we can perform using Copy Activity?\n5. What ADF Activities you have used often in your previous project?",
    "summary": "ADF Properties",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "HCL Technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SQL:\n\nFind all the employees who has salary that is higher than the average salary of their department\n\nemp_id | salary | dep_id\n1           25000     1\n2           30000      1\n3           45000      2\n4           22000      2\n5           50000      3\n\nOutput:\n\nemp_id | dep_id\n2              1\n3              2\n5              3",
    "summary": "Employees salary average",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "HCL Technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "The Position is for Azure Datafactory developer.\n1. How to read the latest modified file from ADLS( Explain what activities you will use and walk through the process)\n2. How will you ingest 1 billion records from oracle Source to ADL\n3. Source has 1 billion records, how will you perform incremental load using ADF and store data in ADL.\n4. we have one on-prem oracle database, explain the end to end process on how will you connect to it and move data to ADL.",
    "summary": "ADL operations",
    "difficulty": "Hard",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "HCL Technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SQL: \nproject_details:\n \nproject_id\temp_id\n1\t\t\t1     3\n1\t\t\t2     1\n1\t\t\t3     2\n2\t\t\t1     3\n2\t\t\t4     1\n \nemp_details:\n \nemp_id\t      emp_name\t                   exp_years \n1\t\tJoe\t\t\t\t3\n2\t\tDaniel\t\t\t        1\n3\t\tAndrew\t\t\t        2\n4\t\tTim\t\t\t\t3\n \nFind the employee who's having highest years of experience in each project\n\nOuput: \n\nproject_id  | emp_id | emp_name | exp_years\n1            1      Joe       3\n2            4       Tim      3",
    "summary": "employee highest experience",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "HCL Technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a table emloyeelogin which has columns empid, time, state\nthe state has 2 values IN and OUT. time is the time at which the employee has either entered or exited the office. Also it is confirmed first he will enter and then exit and he can do this multiple times. We need to find the total time for each emloyee till the time he is in office",
    "summary": "Employee office engage",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You have a spark job running fine but one day it stops and is taking too long to run . How will you approach to solve this issue",
    "summary": "Spark error solution",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a string s and a dictionary of strings wordDict, return true if s can be segmented into a space-separated sequence  \nof one or more dictionary words. \nNote that the same word in the dictionary may be reused multiple times in theÂ segmentation.\n\nInput: s = \"walmart\", wordDict = [\"wal\",\"mart\"] \nOutput: true \nExplanation: Return true because \"walmart\" can be segmented as \"wal mart\". \n\nInput: s = \"walmart\", wordDict = [\"wal\",\"marts\"] \nOutput: false\n\nExample 2: \nInput: s = \"applepenapple\", wordDict = [\"apple\",\"pen\"] \nOutput: true \nExplanation: Return true because \"applepenapple\" can be segmented as \"apple pen apple\". \nNote that you are allowed to reuse a dictionaryÂ word.",
    "summary": "Dictionary words segmentation",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "you have 2 tables employees and department.\nEmployee has following columns: employee_id (Primary Key),name,department_id,salary\nDepartment has following column : department_id (Primary Key),department_name\nWrite an SQL query to find the second highest salary in each department.",
    "summary": "Second highest salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a list of integers. Write a Python program to rearrange the list such that all the zeroes are moved to the end of the list while maintaining the relative order of the non-zero elements. The rearrangement must be done in-place, meaning you cannot use any additional lists or data structures for storage.\nInput : [1, 2, 0, 5, 0, 0, 3]\nOutput : [1,2,5,3,0,0,0]",
    "summary": "List re-arrangement",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Why are you using memory tables in mysql to connect to Looker studio. When you can store your data in HDFS also?",
    "summary": "Memory management mysql",
    "difficulty": "Medium",
    "category": "project based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Cognizant",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1 What will be the result of below - Count(col1), count(*), count(1), count(distinct col1)\n\ncol1 \n12\nabc\ndef\nabc\nSpark\nnull\nnull\nxyz\nnull",
    "summary": "Count function scenario",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantiphi",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "2 \tCan we rollback Truncate and Drop",
    "summary": "Rollback function",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantiphi",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given a file with a lot of paragraphs, \n\tWrite code to find the Total number of Uppercase letters",
    "summary": "Uppercase letters",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Quantiphi",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a function to count the frequency of each character in a string and return a dictionary.\n\na = \"seveneleven\"",
    "summary": "Frequency of character",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Seven Eleven",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Write a function to find all unique substrings of a given length in a string.\n \ninput_string = \"abcabc\"\nsub_str_len = 3\n \n['abc', 'bca', 'cab']",
    "summary": "Finding Unique substrings",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Seven Eleven",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Match \n\nTeam1 team2 winner\nA\tC\tC\nB\tD\tB\nD\tA\tA\nC\tB\tB\nE\tC\tE\n \n  Query to find the total number of matches\n \n Team matches_played #_wins #_loss",
    "summary": "Total matches query",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Seven Eleven",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "How will you design an app like Split Wise on GCP? Create a High level architecture diagram. The interviewer wanted to asses if the candidate knew the different components for full stack development of an everyday App and wanted to asses the knowledge of different services in GCP.",
    "summary": "Split wise Design",
    "difficulty": "Medium",
    "category": "others",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Sigmoid",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Given a string s containing just the characters '(', ')', '{', '}', '[' and ']', determine if the input string is valid.\n\nAn input string is valid if:\n\nOpen brackets must be closed by the same type of brackets.\nOpen brackets must be closed in the correct order.\nEvery close bracket has a corresponding open bracket of the same type.\n\nExample 1:\n\nInput: s = \"()\"\n\nOutput: true\n\nExample 2:\n\nInput: s = \"()[]{}\"\n\nOutput: true\n\nNote - This is a direct question from Leetcode.",
    "summary": "Special Characters",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Sigmoid",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "1) Explain the end to end architecture of project. Explain the ingestion mechanism, the different processing layers, the Data Quality checks, the platform used. Asked to explain the different Data Quality checks implemented.\n2) Compare using Databricks for a Data warehousing projects versus BigQuery. What are the pros and cons of each solution? Why would you choose one over the other? \n3) Spark internal questions like how to choose the cluster configuration in Databricks - Given a 10GB file, explain your thought process behind choosing a Cluster configuration\n4) What is the small file problem? How do you overcome the same in Databricks. How is the same done in BigQuery?",
    "summary": "Project DE",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "40L-50L",
    "companyName": "Sigmoid",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Streaming Data Pipeline Design\n\nYour company collects clickstream data from millions of users across multiple platforms. You need to design a real-time data pipeline that ingests, processes, and stores this data efficiently.\n\nHow would you design this pipeline using GCP services?\nWhat technologies would you use for ingestion, transformation, and storage?\nHow would you handle late-arriving events and out-of-order data?",
    "summary": "Streaming data pipeline design",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Goldman Sachs Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Optimizing Joins in PySpark\n\nYou have two large datasets:\n\norders (100M rows) with columns: order_id, customer_id, order_date, amount.\ncustomers (10M rows) with columns: customer_id, name, email.\nYou need to join them efficiently to get the total amount spent by each customer.\n\n1. What type of join strategy would you use?\n2. How would you optimize performance for large-scale joins in PySpark?\n3. How would broadcast joins help, and when should they be avoided?",
    "summary": "Join optimizations",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Goldman Sachs Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Designing a Data Lake & Warehouse for a FinTech Company\n\nA FinTech startup wants to build a data lake for storing raw transaction logs and a data warehouse for analytics and reporting.\n\nRequirements:\nIngest data from multiple sources (Kafka, APIs, Cloud Storage).\nStore raw data in a Data Lake (GCS/S3) and process it into a structured format.\nUse a data warehouse (BigQuery/Snowflake) for BI & dashboards.\nImplement data governance, access control, and security.\n\nQuestions:\nHow would you design the data lake and warehouse architecture?\nWhat file formats (Parquet, Avro, JSON) would you use for efficient querying?\nHow would you partition, cluster, and optimize queries in BigQuery/Snowflake?\nHow would you ensure compliance with security standards (GDPR, PII masking, encryption)",
    "summary": "Design Fintect pipeline",
    "difficulty": "Hard",
    "category": "project based",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Goldman Sachs Group",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the list of numbers add 1 to the number and return ans in the form of list.",
    "summary": "Numbers with addition",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q . Given the table information like this asked to get the difference\tbtw them\n\nTable - employee\n\nDate \t\t type\t\tfruits\t\tcost\n2025-01-01\t   1\t\tmango\t100\n2025-01-01\t   2\t\tapple\t\t50\n2025-01-02\t   1\t\tmango\t70\n2025-01-02\t   2\t\tapple\t\t120\n2025-01-03\t   1\t\tmango\t95\n\nHe wants results like below\n\nDate\t\t\tcost(difference abs(mango - apple)\n2025-01-01\t\t50\n2025-01-02\t\t50\n2025-01-03\t\t95",
    "summary": "Table difference list",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "2) DSA:-  Remove most consecutive characters â€” used stack",
    "summary": "Consecutive characters removal",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Q - Top K frequent elements from leetcode - I have solved this",
    "summary": "leetcode Frequent elements",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "1) SQL:- pivot the given table below\n\nTable: Sales\nAssume you have a table called sales with the following structure:\n\nElectronics\tTV\t1000\nElectronics   Radio 500\nFurniture\t     Sofa\t1500\n\nFurniture\t    Table\t700\n\nElectronics\t1000\t500\t   0\t               0\nFurniture\t        0\t     \t0\t    1500\t     700\n\n\nDepart ,prod, amountâ€”\n\nOutput - dept , TV,Radio,Sofa,Table",
    "summary": "Electronics sales",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Q - Consecutive Numbers From Leetcode SQL 50\nQ - Exchange Seats from Leetcode SQL 50",
    "summary": "Numbers, seats in Leetcode",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "Paytm",
    "yoe": "<2",
    "role": "Data Engineer-1"
  },
  {
    "text": "Your team has a Data Lake storing structured and semi-structured data. A new field is added to incoming JSON data. How will you handle schema evolution?",
    "summary": "New column JSON",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Value Labs",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Your Kafka-to-Spark Streaming pipeline processes fuel transaction logs from multiple refineries. Suddenly, you notice that 10% of the incoming data contains NULL values in critical fields like fuel_quantity . How will you handle this in real time without stopping the pipeline?",
    "summary": "NULL values scenario",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Value Labs",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "You have a Spark job that processes 1 TB of data daily and is taking 5 hours to complete. How will you optimize it?",
    "summary": "Job optimization",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "30L-40L",
    "companyName": "Value Labs",
    "yoe": "8-12",
    "role": "Lead Data Engineer"
  },
  {
    "text": "Write a program to find the length of longest substring without repeating characters\nInput - 'abcabd'\nOutput - 3",
    "summary": "Find repeating characters",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Bain & Company",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a program to generate a list with random numbers, given an integer 'n'. The list should contain equal number of negative and positive numbers\nInput - 4\nOutput - [1, -1, 2, -2]\n\nInput - 3\nOutput - [1, -1, 0]",
    "summary": "Random numbers generation",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Bain & Company",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How will you decrease cluster costs when you're processing large amounts of data in Spark?",
    "summary": "Spark data optimization",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Bain & Company",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a program to generate fibonacci numbers that fall between the two given integers:\nInput : a=20, b=80\nOutput : [21, 34, 55]",
    "summary": "Fibonacci number",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "ShopSe",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a program to find common prefix between the strings in a given list:\nInput: ['listy', 'lisriu', 'lisbon']\nOutput: 'lis'",
    "summary": "Find common Prefix",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "ShopSe",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How to tackle the data skew problem?",
    "summary": "Data skew",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "ShopSe",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find factorial of a number in python\n\nSolution: (using recursion)\nFactorial of a number\n\ndef factorial(n):\n\tif n==0 or n==1 :\n\t\treturn 1\n\telse:\n\t\treturn n*factorial(n-1)",
    "summary": "Factorial of number",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Design a query that identifies the top 5 students based on their scores from a Students table. select only those students who are in the top 5 ranks.\nSolution:\n\nWITH cte AS (\n    SELECT \n        StudentID, \n        Name, \n        Score,\n        RANK() OVER (ORDER BY Score DESC) AS rank\n    FROM Students\n)\nSELECT *\nFROM cte\nWHERE rank <= 5\n\nSample Input (Example Students Table):\n| StudentID | Name      | Score |\n|-----------|-----------|-------|\n| 1         | Alice     | 95    |\n| 2         | Bob       | 88    |\n| 3         | Charlie   | 92    |\n| 4         | David     | 85    |\n| 5         | Eve       | 90    |\n| 6         | Frank     | 87    |\n| 7         | Grace     | 93    |\n| 8         | Henry     | 89    |\n\nSample Output (Top 5 Students):\n| StudentID | Name      | Score | rank |\n|-----------|-----------|-------|------|\n| 1         | Alice     | 95    | 1    |\n| 3         | Charlie   | 92    | 2    |\n| 7         | Grace     | 93    | 3    |\n| 5         | Eve       | 90    | 4    |\n| 2         | Bob       | 88    | 5    |",
    "summary": "Top 5 ranks of students",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Question:\nGiven a table with a sequence of numbers, find the missing numbers in the range 1 to the maximum number present in the table.\n\nSolution:\nSample Table (Numbers)\nID\n1\n2\n3\n5\n6\n8\nExpected Output (Missing Numbers in Sequence)\nMissing_Number\n4\n7\n\n\nWITH NumberSeries AS (\n    SELECT ROW_NUMBER() OVER (ORDER BY (SELECT NULL)) AS Num\n    FROM master.dbo.spt_values  -- Generates a sequence of numbers (SQL Server)\n)\nSELECT Num AS Missing_Number\nFROM NumberSeries\nLEFT JOIN Numbers N ON NumberSeries.Num = N.ID\nWHERE Num BETWEEN 1 AND (SELECT MAX(ID) FROM Numbers) -- Restricting range\nAND N.ID IS NULL;",
    "summary": "Missing number",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Tech Mahindra",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Problem Statement:\nYou are given an array of integers arr, its size n, and a number k. The arguments are passed in the order: n, arr, k.\n\nWrite a function findMinMaxSum that computes:\n\nThe maximum possible element in the array after discarding exactly k elements.\nThe minimum possible element in the array after discarding exactly k elements.\nReturn these two values in an array [max, min].\n\nConstraints:\nYou must not use any built-in sorting library function.\nk < n (ensuring at least one element remains in the array after removing k elements).",
    "summary": "findMinMaxSum python",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "NeoStats Analytics Solutions.",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a string s of length n and an integer k. The arguments are passed in the order: n, s, k.\n\nWrite a function findMinMaxChar that:\n\nFinds the lexicographically largest character in the string after removing exactly k characters.\nFinds the lexicographically smallest character in the string after removing exactly k characters.\nReturn these two characters as an array [maxChar, minChar].\n\nConstraints:\nYou must not use any built-in sorting functions.\nk < n (ensuring at least one character remains in the string after removing k characters).",
    "summary": "findMinMaxChar python",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "NeoStats Analytics Solutions.",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a table named sales with the following columns:\n\nColumn Name\tData Type\tDescription\nid\tINT\tUnique identifier for each sale\nname\tVARCHAR\tName of the employee\nmonth\tVARCHAR\tMonth of the sale\nsales\tINT\tSales amount for that month\nSample Data:\nid\tname\tmonth\tsales\n1\txyz\taug\t2000\n2\tabc\tsept\t3000\n1\txyz\tjan\t3000\n2\tabc\tfeb\t4000\n1\txyz\tfeb\t3000\n3\text\tjan\t3000\nQuestion:\nWrite an SQL query to find the top 5 employees based on their total sales across all months.\nIf multiple employees have the same sales amount, they should have the same rank.\n\nUse the RANK() window function to assign ranks based on total sales, and return only the top 5 ranked employees.\n\nExpected Output Format:\nname\ttotal_sales\trank\nxyz\t8000\t1\nabc\t7000\t2\next\t3000\t3",
    "summary": "Top 5 employees sales",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "5L-10L",
    "companyName": "NeoStats Analytics Solutions.",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Input = [2,6,8,9]\nTarget = 11\nfind possible matches and do it in python/scala",
    "summary": "Possible matches python",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM System",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "spark optimization techniques",
    "summary": "Spark optimization",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM System",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "find duplicates from list using scala/python\n[1,2,3,3,4,6,8]",
    "summary": "Duplicates in list",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "EPAM System",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write a program to sort array in ascending order.",
    "summary": "Sort Array",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are traits, Case Class in Scala?",
    "summary": "Scala",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Explain what use of Option[T] and Some[T] is in Scala, and how it helps in handling null or missing values.",
    "summary": "Scala",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a SQL query/Spark code to calculate and populate how much hours employee spent inside office for a specific day.\n\ne.g. Input Table\nEmp_id status created\nEMP0001 check_in 2024-11-22 08:00:00\nEMP0002 check_in 2024-11-22 08:00:50\nEMP0001 check_out 2024-11-22 13:00:00\nEMP0002 check_out 2024-11-22 13:01:00\nEMP0001 check_in 2024-11-22 13:31:00\nEMP0002 check_in 2024-11-22 13:33:50\nEMP0001 check_out 2024-11-22 18:03:00\nEMP0002 check_out 2024-11-22 18:04:00\n\nExpected Output:\nEmp_id created time_spend_in office\nEMP0001 2024-11-24 08:00:00 09:32:00\nEMP0002 2024-11-24 08:00:50 09:30:20",
    "summary": "Employee working hours",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What is Singleton and Companion Objects in Scala.",
    "summary": "Scala",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are design patterns you are aware of in scala and also explain them?\ne.g. Singleton Pattern, Factory Pattern",
    "summary": "Design patterns scala",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write Program to find all the permutations of a string using recursion.\n\ne.g\nInputStr = \"ABC\"\n\noutput:\nABC, ACB, BAC, BCA, CBA, CAB",
    "summary": "Permutation of a String",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a program read/query data from any type SQL database and write data to a output file.\n\ne.g. Say you have MYSQL DB, write script to perform following steps:\n1) Use any Library/module of your choice\n2) establish connection to DB using connector\n3) execute SQL query\n4) Fetch the query output and store in a file.\nType: problem solving",
    "summary": "SQL Write & Read",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "UBS",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Consider there is a OLTP data like MySQL. you need to bring the data from that database to a data warehouse in near realtime. Assume the compute and storage is not a problem. And the data will be transferred in during business hours and there is a lot of read operations that happens during that time. Wht would be your approach?",
    "summary": "Design OLTP system",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "California Govt",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are designing a batch ETL pipeline that processes daily sales data from an S3 bucket and loads it into a Snowflake data warehouse. The raw data files are in CSV format and contain duplicate records due to data ingestion issues.\n\n\nHow would you design the batch pipeline to efficiently process and deduplicate the data before loading it into Snowflake?\nWhat strategies would you use to handle late-arriving data in batch processing?\nWrite a SQL or Spark query to remove duplicates based on the order_id before inserting the data into Snowflake.",
    "summary": "Design ETL sales pipeline",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "California Govt",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You have a scenario where your database performance is degrading. What would be your approach at database tier and at sever level? How would you approach this problem in solving this?",
    "summary": "Database performance degrade",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "United States",
    "ctc": "70L-80L",
    "companyName": "California Govt",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find total number of clocked hours for each employee (inside the office) Flag I means punch in and O means punch out. Employee can do multiple punch in and punch out. For each punch in there will be a punch out.\n input -\nEmp ID  Time    Flag\n114         8:30    I\n114        10:30    O\n114        11:30    I\n114        15:30    O\n115        9:30      I\n115        17:30    O\n\noutput -\n\nEmpId Time\n114    6hrs\n115    8hrs",
    "summary": "Employee working hours",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Exl",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "spark optimization techniques and dif between repartition and coalesce",
    "summary": "Spark optimization techniques",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Exl",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "input - ['aet','tea','bet','teb','ebt']\noutput - ['aet',tea] , ['teb',ebt','tbe']\nfind all anagrams and group similar words together in a list using scala / python",
    "summary": "Anagrams grouping",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Exl",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Write query to Swap the names of students in pairs such that students with odd id get the name of the next id, and students with even id get the name of the previous id.\n\nGiven Student Table\n\nid\t\tname\n1\t\ta\n2\t\tb\n3\t\tc\n4\t\td\n\nExpected output\nid \tname\n1\tb\n2\ta\n3\td\n4\tc",
    "summary": "Swap student names",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Ecom Express",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given Cluster Configuration:6 node, 16 core, 64gb ram \nFind the number of executors and each executor memory",
    "summary": "Cluster configuration",
    "difficulty": "Easy",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Ecom Express",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given two teams. Write SQL Query to find the number of match played between different teams.\nSample Dataset\n\nteam1\tteam2\na\tb\na\tc\na\td\nb \tc",
    "summary": "Matches Played",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Ecom Express",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given Device Table as below:\n\nDEVICE_ID\tHOST_ID\t\tPAIRING_TIME\nBT1\t\tTV1\t\t3PM\nBT1\t\tMob1\t\t9AM\nBT1\t\tLP1\t\t6PM\nUSB1\t\tMob1\t\t12PM\n\nWrite sql query to return\nDEVICE_ID\tFIRST_PAIRED_HOST\tFIRST_PAIRED_TIME\tLATEST_PAIRED_HOST\tLATEST_PAIRED_TIME",
    "summary": "Bluetooth paired devices",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "GlobalLogic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given Sales Table with Columns:\t\t\t\nitem\tcategory\tyear\tqty_sold\tprice_per_unit\n\nWrite sql query to return\ncategory, year, year_growth\n\nGiven \nrevenue = qty_sold * price_per_unit",
    "summary": "Growth on year basis",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "GlobalLogic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given two tables as below\nA\t\tB\nID\t\tID\n\n1\t\t1\n1\t\t1\n1\t\t1\n1\t\t2\n1\t\t3\n1\t\t4\n\nTell me the number of count of all joins (inner, left, outer, full outer)",
    "summary": "SQL Joins",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "GlobalLogic",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. Different Types of Join Strategies in Spark\n2. How to optimize a spark job?",
    "summary": "Spark DE",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Elanco",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "SQL Question\nGiven the reviews table, write a query to retrieve the average star rating for each product, grouped by month. The output should display the month as a numerical value, product ID, and average star rating rounded to two decimal places. Sort the output first by month and then by product ID.\n\nreviews table - review_id, user_id, product_id, review_date, stars\nWas also asked to write the solution in Pyspark as well.",
    "summary": "Average product rating",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Elanco",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Python Problem\n\nYou are given two lists:\nexpectations: A list where each element represents the minimum card value a person expects.\ncards: A list of available card values.\nEach person can be satisfied if they receive a card with a value greater than or equal to their expectation. However, a card can only be assigned to one person.\n\nYour task is to determine the maximum number of people who can be satisfied by distributing the available cards optimally.\n\nexpectations = [5, 10, 1000]  \ncards = [10, 15, 100] \n\noutput - 2",
    "summary": "Distribute available cards",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Elanco",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a list of dictionaries where each dictionary represents a product with keys: product_id, category, and price. Write a function to calculate the total price of products in each category and return the results as a dictionary.\n\nInput- products = [\n{\"product_id\": 1, \"category\": \"Electronics\", \"price\": 299.99},\n{\"product_id\": 2, \"category\": \"Electronics\", \"price\": 199.99},\n{\"product_id\": 3, \"category\": \"Clothing\", \"price\": 49.99},\n{\"product_id\": 4, \"category\": \"Clothing\", \"price\": 19.99},\n{\"product_id\": 5, \"category\": \"Books\", \"price\": 9.99}\n]\n\noutput:\n{\n\"Electronics\": 499.98,\n\"Clothing\": 69.98,\n\"Books\": 9.99\n}",
    "summary": "Total product price",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "GIST Impact",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a table with employee salary data recorded at different quarters. However, some salary values are missing (NULL or empty).\nYour task is to write an SQL query that fills the missing salary values by carrying forward the last known salary from the previous quarter for each id.\n\nAttached the sample input and output.",
    "summary": "Last known salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "GIST Impact",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "You are given a table containing id and cost values. Your task is to compute the running total (cumulative sum) of cost for each id.\n\nAdditionally, analyze the behavior of NULL values in the id column when using window functions with PARTITION BY id.\n\nid cost\n1 10\n2 20\n2 30\nnull 10\n3 20\n3 30",
    "summary": "cumulative sum of product",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "GIST Impact",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. How to optimize a long-running pipeline in Azure Data Factory?\n2. How is the logging and alert mechanism set up in ADF in your project?\n3. Different types of integration runtimes in ADF and when do you use them?",
    "summary": "Azure DE",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "GIST Impact",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a Python program to determine the minimum characters that need to be added to a given string to make it a palindrome.\n\nexample:\nInput: \"abcd\"\nOutput: 3 (\"dcb\" needs to be added, resulting in \"abcdcba\")",
    "summary": "Palindrome character addition",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Amazon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Table Routes\nStops\n1\n2\n3\n6\n8\n9\n\n\nWrite an SQL query to identify the continuous delivery segments where the driver delivered without skipping a stop. The output should display the start and end stops of each continuous sequence.\n\noutput\nstart         end\n1               3\n8               9",
    "summary": "Driver package delivery",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Amazon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "How can you efficiently join two large tables while optimizing performance?",
    "summary": "Spark optimization",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Amazon",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given a list of countries:\nIndia\nSri Lanka\nBangladesh\nPakistan\nEach country needs to play against every other country once. Generate the output where each match consists of Team A vs Team B",
    "summary": "Matches between countries",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Broadridge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Executor configuration :\nMemory per executor : 13GB\nCores per Executor : 5 cores\nData processing speed : 256MB of data processed in 20 seconds\nFind total tasks, ideal executors, tasks in parallel and execution time for a 1TB file",
    "summary": "Spark configuration",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Broadridge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Find the largest common prefix for the list of strings given below:\n[â€œgeeksforgeeksâ€, â€œgeeksâ€, â€œgeekâ€, â€œgeezerâ€]",
    "summary": "Largest prefix",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Broadridge",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "a=[1,2,3,4,5,5,4,3,2,4,6,7,8,12,11,10,11,45,6,7,3]\nWrite python programme to find the count of occurances",
    "summary": "Occurance of numbers",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "scientist technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write python/pyspark code to ingest the data(20GB) which has integer data. Load the data into another source in ascending format",
    "summary": "Data load in ascending order",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "scientist technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Explain about role and responsibilies in your current project",
    "summary": "Project DE",
    "difficulty": "Easy",
    "category": "project based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "scientist technologies",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "nums = [{\"dt\":\"2024-08-01\",\"stock\":\"abcd\",\"price\":\"245\",\"volume\":\"1500\"},\n{\"dt\":\"2024-08-02\",\"stock\":\"abcd\",\"price\":\"235\",\"volume\":\"1800\"},\n{\"dt\":\"2024-08-03\",\"stock\":\"abcd\",\"price\":\"240\",\"volume\":\"1400\"},\n{\"dt\":\"2024-08-04\",\"stock\":\"abcd\",\"price\":\"252\",\"volume\":\"1200\"},\n{\"dt\":\"2024-08-05\",\"stock\":\"abcd\",\"price\":\"265\",\"volume\":\"1000\"},\n{\"dt\":\"2024-08-06\",\"stock\":\"abcd\",\"price\":\"215\",\"volume\":\"1000\"}]\n\n\nFind the following - \n1) min_price\n2) max_price\n3) avg_volume\n4) max_profit (buy first then sell)",
    "summary": "Stock price analysis",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Games24x7",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "games -\n+---------------------+---------+---------+----------+------------+\n| tstamp              | user_id | game_id | win_flag | points_won |\n+---------------------+---------+---------+----------+------------+\n| 2024-08-01 00:00:01 |    1000 |       1 |        1 |         50 |\n| 2024-08-02 11:01:00 |    1001 |       2 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       3 |        1 |         30 |\n| 2024-08-03 14:53:01 |    1000 |       4 |        0 |          0 |\n| 2024-08-03 13:51:01 |    1001 |       5 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1005 |       7 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1006 |       8 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       9 |        1 |         10 |\n| 2024-08-02 15:01:00 |    1001 |      10 |        1 |         20 |\n+---------------------+---------+---------+----------+------------+\n\n\na) Daily total no. of games played\n\nExpected output - \n+------------+-------------+\n| dt         | total_games |\n+------------+-------------+\n| 2024-08-01 |           1 |\n| 2024-08-02 |           2 |\n| 2024-08-03 |           6 |\n+------------+-------------+",
    "summary": "Total games played",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Games24x7",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "games -\n+---------------------+---------+---------+----------+------------+\n| tstamp              | user_id | game_id | win_flag | points_won |\n+---------------------+---------+---------+----------+------------+\n| 2024-08-01 00:00:01 |    1000 |       1 |        1 |         50 |\n| 2024-08-02 11:01:00 |    1001 |       2 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       3 |        1 |         30 |\n| 2024-08-03 14:53:01 |    1000 |       4 |        0 |          0 |\n| 2024-08-03 13:51:01 |    1001 |       5 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1005 |       7 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1006 |       8 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       9 |        1 |         10 |\n| 2024-08-02 15:01:00 |    1001 |      10 |        1 |         20 |\n+---------------------+---------+---------+----------+------------+\n\n\nb) Top 3 users having highest win ratio\n\nExpected output - \n+---------+-----------+\n| user_id | win_ratio |\n+---------+-----------+\n|    1003 |    1.0000 |\n|    1000 |    0.5000 |\n|    1001 |    0.3333 |\n+---------+-----------+",
    "summary": "Highest win ratio",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Games24x7",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "games -\n+---------------------+---------+---------+----------+------------+\n| tstamp              | user_id | game_id | win_flag | points_won |\n+---------------------+---------+---------+----------+------------+\n| 2024-08-01 00:00:01 |    1000 |       1 |        1 |         50 |\n| 2024-08-02 11:01:00 |    1001 |       2 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       3 |        1 |         30 |\n| 2024-08-03 14:53:01 |    1000 |       4 |        0 |          0 |\n| 2024-08-03 13:51:01 |    1001 |       5 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1005 |       7 |        0 |          0 |\n| 2024-08-03 00:53:01 |    1006 |       8 |        0 |          0 |\n| 2024-08-03 12:53:00 |    1003 |       9 |        1 |         10 |\n| 2024-08-02 15:01:00 |    1001 |      10 |        1 |         20 |\n+---------------------+---------+---------+----------+------------+\n\n\nc) Leaderboard based on total points won, \nin case of tie consider total number of games played",
    "summary": "Players leaderboard",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Games24x7",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given an input string \"oneplustwominusonemultiply3\" convert it into \"1+2-1*3\" and also generare output of arthemetic operation \"3\" in o(n) time complexity",
    "summary": "Arithmetic operation",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "JPMorgan Chase",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "find the players in olympic table who has won only gold medal and not any other medal in sql ?",
    "summary": "Olympic players with gold medals",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "JPMorgan Chase",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "What are differnet spark optimization techniques used in your project and when would you decide which to use and how has it helped to improve the spark job performance",
    "summary": "Spark optimizations",
    "difficulty": "Medium",
    "category": "project based",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "JPMorgan Chase",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Given an pair of array of string find the common prefix in all the strings and if the next character in string is same as matching character it should be in ouput\ninput : (\"sunnday\",\"sunden\") output : sunnd\ninput : (\"sunnday\",\"suneden\") output : sunn",
    "summary": "Common prefix in strings",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Expedia",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "given an array of values find minimum subarray containing k unique values ?\ninput : arr = [2, 2, 1, 1,3,2 ] and k =3\noutput: Minimum subarray containing all unique values: [1, 3, 2]",
    "summary": "Min subarray with unique values",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Expedia",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "explain adaptive query execution and what its befits and elaborate in detail in spark ?",
    "summary": "Adaptive query execution",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "United Kingdom",
    "ctc": "60L-70L",
    "companyName": "Expedia",
    "yoe": "5-8",
    "role": "Senior Data Engineer"
  },
  {
    "text": "Spark and Databricks Interview Questions - Fractal Interview\n\n1. What is the difference between Spark's DataFrame and RDD? When should you use one over the other?\n2. Explain how Spark handles data shuffling. What are some ways to optimize shuffle performance?\n3. How does the Catalyst Optimizer improve query performance in Spark?\n4. In Databricks, how would you efficiently process a large dataset stored in Delta Lake with partitioning and Z-order indexing?\n5. How does Adaptive Query Execution (AQE) in Spark 3 improve query performance? Provide a real-world use case where AQE can be beneficial.",
    "summary": "spark and databricks Theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Fractal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1. Your database contains millions of records, and a report query is running slowly. How would you optimize it?\n2. A banking application needs to track customer transactions in real-time. How would you design the SQL queries and database schema?\n3. You need to migrate a large table from one database to another without downtime. How would you approach it?\n4. In a retail database, how would you identify customers who havenâ€™t made a purchase in the last 12 months?\n5. How would you implement Slowly Changing Dimensions (SCD) in SQL for a data warehouse environment?",
    "summary": "DE scenario",
    "difficulty": "Hard",
    "category": "scenario based",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Fractal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find the Second Highest Salary from an Employee Table.\n\nTable Creation:\n\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(50),\n    salary INT\n);\n\nSample Input Data:\n\nINSERT INTO Employee (id, name, salary) VALUES\n(1, 'Alice', 90000),\n(2, 'Bob', 85000),\n(3, 'Charlie', 87000),\n(4, 'David', 92000),\n(5, 'Eve', 95000);\n\n\nExpected Output:\n\n| Second_Highest_Salary  |\n|---------------------------|\n| 92000                            |",
    "summary": "Second highest salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Fractal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find Employees Who Earn More Than Their Manager\n\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(50),\n    salary INT,\n    manager_id INT\n);\n\nINSERT INTO Employee (id, name, salary, manager_id) VALUES\n(1, 'Alice', 90000, 3),\n(2, 'Bob', 85000, 3),\n(3, 'Charlie', 87000, NULL),\n(4, 'David', 95000, 3),\n(5, 'Eve', 98000, 4);\n\nExpected Output:\n\n| name  | salary |\n|-------|--------|\n| David | 95000  |\n| Eve   | 98000  |",
    "summary": "Employees earn more than manager",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "20L-30L",
    "companyName": "Fractal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1.What is normalization, and how do you perform normalization up to the 3rd Normal Form (3NF)?\n2.What is the difference between primary keys and foreign keys in relational databases?\n3. How would you design a database schema for an e-commerce system to store product, order, and customer information?",
    "summary": "DE theory",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1.You are given a list (or array) of integers in Python, and you need to find the largest integer in that array.\n\nInput Data Creation \n\n#Python\n\ninteger_array = [10, 5, 20, 8, 25, 12]  \n\nExpected Output\nThe output should be 25.",
    "summary": "Largest number in array",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "1.You need to process a massive log file in Hadoop, where each line represents a user interaction. How would you structure this job to run efficiently on a Hadoop cluster?\n2.Describe how the Hadoop Distributed File System (HDFS) handles data replication and fault tolerance ?\n3.You have a large dataset containing user data and their transactions. How would you process and analyze this dataset in Spark using Data Frames? What operations would you use?",
    "summary": "DE scenario",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Find the Third Highest Salary from an Employee Table\n\nTable Creation:\nCREATE TABLE Employee (\n    id INT PRIMARY KEY,\n    name VARCHAR(50),\n    salary INT\n);\n\nSample Input Data:\n\nINSERT INTO Employee (id, name, salary) VALUES (1, 'Avani', 90000),\n(2, 'Rohan', 85000),(3, 'Chaitanya', 87000), (4, 'Dheeraj', 93000),(5, 'Eesha', 95000),\n(7, 'Gaurav', 80000),(9, 'Ishita', 75000),(10, 'Karan', 98000),(12, 'Liam', 70000);\n\nExpected Output:\n\nThird Highest Salary \n\n| Name  |  Salary                 |\n|----------------------------|\n| Dheeraj|     93000            |",
    "summary": "3rd highest salary",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Wipro",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Among AWS, Azure, and GCP, Which cloud platform will you suggest to the client for building scalable data pipelines and Why ?",
    "summary": "Cloud services",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neenopal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Write a query to find all the employees who draw more salary than their respective department's average salary\n\nId     Name          Dept ID   Dept Name   Salary\n1      Alex             02             Finance           20000\n2      Bob              01             Marketing      17500\n3     Carlson        02             Finance            28000\n4     David            03             HR                     15000\n5     Elizabeth     02             Finance            22000\n6     Felix              01             Marketing       30000\n7     Griffith         01             Marketing       26000\n8    Harley            03            HR                     19000\n9    Jaden            03             HR                     15000",
    "summary": "Employees drawing more salary",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neenopal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Given the below data frame, create a new data frame to show the subjectwise score obtained by the students.\n\nId   Name          Marks\n1    Ajay              32|45|39\n2    Jay                36|50|43\n3    Rohit            42|40|48\n\n\nThe output should have following columns - Id, Name, Physics, Chemistry, Maths",
    "summary": "Student subject scores",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Neenopal",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Word Search\n\nGiven an m x n grid of characters board and a string word, return true if word exists in the grid.\nThe word can be constructed from letters of sequentially adjacent cells, where adjacent cells are horizontally or vertically neighboring. The same letter cell may not be used more than once.",
    "summary": "Character board string",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "United States",
    "ctc": ">1cr",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Valid Anagram\n\nGiven two strings s and t, return true if t is an anagram of s, and false otherwise.",
    "summary": "Valid anagram",
    "difficulty": "Easy",
    "category": "problem solving",
    "country": "United States",
    "ctc": ">1cr",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "What are decorators in python?",
    "summary": "Decorators in python",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "United States",
    "ctc": ">1cr",
    "companyName": "Walmart",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Below is the sales data for different products over several months.\nWrite a Python Apache beam and perform below operation:\nLoad the data from a CSV file.\nDisplay the total sales for each product.\nFilter out products with total sales less than 500.\nDisplay the top 5 products with the highest sales.\n \n \nproduct,month,sales\nProduct A,2024-01,120 \nProduct B,2024-01,150\nProduct A,2024-02,200\nProduct C,2024-02,300\nProduct B,2024-02,400\nProduct A,2024-03,250\nProduct C,2024-03,350\nProduct B,2024-03,100\nProduct A,2024-04,180\nProduct C,2024-04,200",
    "summary": "Sales Product Data",
    "difficulty": "Hard",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "what are sideinputs in apache beam",
    "summary": "side inputs in apache beam",
    "difficulty": "Easy",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "do you know about hive",
    "summary": "Hive",
    "difficulty": "Easy",
    "category": "others",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "explain about wildcards in bigquery",
    "summary": "wildcards in bigquery",
    "difficulty": "Medium",
    "category": "theoretical",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "sql question on populationstatistics table\nwhere we have population and we have to partition by city into male and female population under age 30 in bq (table not provided) we have to assume the data (city,age,gender,population are given as columns)",
    "summary": "Population Statistics",
    "difficulty": "Medium",
    "category": "problem solving",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  },
  {
    "text": "Can we partition by string in bigquery?",
    "summary": "Parition by string",
    "difficulty": "Medium",
    "category": "scenario based",
    "country": "India",
    "ctc": "10L-20L",
    "companyName": "Impetus",
    "yoe": "2-5",
    "role": "Data Engineer-2"
  }
]